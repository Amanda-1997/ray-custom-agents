{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trpo-ray.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denklewer/ray-custom-agents/blob/master/trpo_ray_ray_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAGWDBl0Cxk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install -y build-essential curl unzip psmisc\n",
        "!pip install cython==0.29.0\n",
        "!git clone https://github.com/ray-project/ray.git\n",
        "!ray/ci/travis/install-bazel.sh\n",
        "!pip install lz4\n",
        "!pip install setproctitle\n",
        "!mv ray ray-distr\n",
        "!pip install -e ray-distr/python/. --verbose  # Add --user if you see a permission denied error.\n",
        "!apt-get install swig\n",
        "!apt install -y python3-dev zlib1g-dev libjpeg-dev cmake swig python-pyglet python3-opengl libboost-all-dev libsdl2-dev \\\n",
        "    libosmesa6-dev patchelf ffmpeg xvfb\n",
        "!pip3 install box2d box2d-kengz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSOf6g3sfOP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(\"\", '/content/ray-distr/python')) # To find local version of the library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u35Ns4e_-Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "def conv2d_size_out(size, kernel_size=3, stride=2):\n",
        "    \"\"\"\n",
        "    common use case:\n",
        "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
        "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
        "    to understand the shape for dense layer's input\n",
        "    \"\"\"\n",
        "    return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_iCFvpCJJLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import gym\n",
        "import logging\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.evaluation import PolicyGraph, PolicyEvaluator\n",
        "from ray.rllib.policy.sample_batch import SampleBatch\n",
        "from ray.rllib.evaluation.metrics import collect_metrics\n",
        "from ray.rllib.models.pytorch.misc import  valid_padding\n",
        "import numpy as np\n",
        "\n",
        "import scipy\n",
        "import scipy.signal\n",
        "from ray.rllib.utils.annotations import override\n",
        "logger = logging.getLogger(__name__)\n",
        "from ray.rllib.evaluation.postprocessing import compute_advantages,Postprocessing\n",
        "\n",
        "from torch.autograd import Variable\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--gpu\", action=\"store_true\")\n",
        "parser.add_argument(\"--num-iters\", type=int, default=20)\n",
        "parser.add_argument(\"--num-workers\", type=int, default=2)\n",
        "from ray.rllib.models.catalog import ModelCatalog, MODEL_DEFAULTS\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class TRPOAgent(nn.Module):\n",
        "    def __init__(self, observation_space, action_space, config,hidden_size=32):\n",
        "        '''\n",
        "        Here you should define your model\n",
        "        You should have LOG-PROBABILITIES as output because you will need it to compute loss\n",
        "        We recommend that you start simple:\n",
        "        use 1-2 hidden layers with 100-500 units and relu for the first try\n",
        "        '''\n",
        "        nn.Module.__init__(self)\n",
        "        config = defaultdict(**config)\n",
        "        self.config = config\n",
        "        self.n_actions = action_space.n\n",
        "        self.state_shape = observation_space.shape\n",
        "        self.action_space =action_space\n",
        "        self.observation_space = observation_space\n",
        "        self.dist_class, self.logit_dim = ModelCatalog.get_action_dist(\n",
        "        self.action_space, None, torch=True)\n",
        "\n",
        "                \n",
        "        if isinstance(self.observation_space, gym.spaces.Discrete):\n",
        "            self.obs_rank = 1\n",
        "        else:\n",
        "            self.obs_rank = len(self.observation_space.shape)\n",
        "            \n",
        "        if self.obs_rank > 1:\n",
        "          w, h, c = (84, 84 , 4)\n",
        "          self.common = nn.Sequential(\n",
        "              nn.Conv2d(c, 32, kernel_size=3, stride=2),\n",
        "              nn.ReLU(),\n",
        "              nn.Conv2d(32, 32, kernel_size=3, stride=2),\n",
        "              nn.ReLU(),\n",
        "              nn.Conv2d(32, 32, kernel_size=3, stride=2),\n",
        "              nn.ReLU(),\n",
        "              Flatten())\n",
        "          convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "          convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "          linear_input_size = convw * convh * 32\n",
        "          # we want to flatten given images to verctor for dense layers.\n",
        "          self.fc = nn.Sequential(\n",
        "              nn.Linear(linear_input_size, 128),\n",
        "              nn.ReLU()\n",
        "          )\n",
        "          self.log_probs = nn.Sequential(\n",
        "              nn.Linear(128, self.n_actions),\n",
        "              nn.LogSoftmax()\n",
        "          )\n",
        "        else:\n",
        "          self.model = nn.Sequential(\n",
        "              nn.Linear(self.state_shape[0], hidden_size),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden_size, self.n_actions),\n",
        "              nn.LogSoftmax()\n",
        "          )\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"\n",
        "        takes agent's observation (Variable), returns log-probabilities (Variable)\n",
        "        :param state_t: a batch of states, shape = [batch_size, state_shape]\n",
        "        \"\"\"\n",
        "        \n",
        "                          \n",
        "        # Use your network to compute log_probs for given state\n",
        "        if self.obs_rank > 1:\n",
        "          states= states.permute(0,3,2,1)\n",
        "          state_t = self.common(states)\n",
        "          state_t = self.fc(state_t)\n",
        "          return self.log_probs(state_t)\n",
        "        else:\n",
        "          return self.model(states)\n",
        "\n",
        "    def get_log_probs(self, states):\n",
        "        '''\n",
        "        Log-probs for training\n",
        "        '''\n",
        "\n",
        "        return self.forward(states)\n",
        "\n",
        "    def get_probs(self, states):\n",
        "        '''\n",
        "        Probs for interaction\n",
        "        '''\n",
        "\n",
        "        return torch.exp(self.forward(states))\n",
        "\n",
        "    def act(self, obs, sample=True):\n",
        "        '''\n",
        "        Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n",
        "        :param: obs - single observation vector\n",
        "        :param sample: if True, samples from \\pi, otherwise takes most likely action\n",
        "        :returns: action (single integer) and probabilities for all actions\n",
        "        '''\n",
        "\n",
        "        probs = self.get_probs(Variable(torch.FloatTensor([obs]))).data.numpy()\n",
        "\n",
        "        if sample:\n",
        "            action = int(np.random.choice(self.n_actions, p=probs[0]))\n",
        "        else:\n",
        "            action = int(np.argmax(probs))\n",
        "\n",
        "        return action, probs[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qxmLg0Gdmqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UTIL FUNCTIONS\n",
        "def get_cummulative_returns(r, gamma=1):\n",
        "  \"\"\"\n",
        "  Computes cummulative discounted rewards given immediate rewards\n",
        "        G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
        "        Also known as R(s,a)\n",
        "  \"\"\"\n",
        "  r = np.array(r)\n",
        "  assert r.ndim >= 1\n",
        "  return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]\n",
        "\n",
        "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
        "    \"\"\"\n",
        "    This method solves system of equation Ax=b using iterative method called conjugate gradients\n",
        "    :f_Ax: function that returns Ax\n",
        "    :b: targets for Ax\n",
        "    :cg_iters: how many iterations this method should do\n",
        "    :residual_tol: epsilon for stability\n",
        "    \"\"\"\n",
        "    p = b.clone()\n",
        "    r = b.clone()\n",
        "    x = torch.zeros(b.size())\n",
        "    rdotr = torch.sum(r * r)\n",
        "    for i in range(cg_iters):\n",
        "        z = f_Ax(p)\n",
        "        v = rdotr / (torch.sum(p * z) + 1e-8)\n",
        "        x += v * p\n",
        "        r -= v * z\n",
        "        newrdotr = torch.sum(r * r)\n",
        "        mu = newrdotr / (rdotr + 1e-8)\n",
        "        p = r + mu * p\n",
        "        rdotr = newrdotr\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioP5xoJbJJZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomPolicy(PolicyGraph):\n",
        "    \"\"\"Example of a custom policy graph written from scratch.\n",
        "    You might find it more convenient to extend TF/TorchPolicyGraph instead\n",
        "    for a real policy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space, action_space, config):\n",
        "        PolicyGraph.__init__(self, observation_space, action_space, config)\n",
        "        # example parameter\n",
        "        self.w = 1.0\n",
        "        self.observation_shape = observation_space.shape\n",
        "        self.n_actions = action_space.n\n",
        "        self.agent = TRPOAgent(observation_space, action_space, config)\n",
        "        self.policy = []\n",
        "        \n",
        "\n",
        "\n",
        "    def compute_actions(self,\n",
        "                        obs_batch,\n",
        "                        state_batches,\n",
        "                        prev_action_batch=None,\n",
        "                        prev_reward_batch=None,\n",
        "                        info_batch=None,\n",
        "                        episodes=None,\n",
        "                        **kwargs):\n",
        "        # return random actions\n",
        "        actions = []\n",
        "        action_probs = []\n",
        "        for obs in obs_batch:\n",
        "            action, policy = self.agent.act(obs)\n",
        "            actions.append(action)\n",
        "            action_probs.append(policy)\n",
        "\n",
        "        return actions, [], {\"action_probs\": action_probs}\n",
        "      \n",
        "    @override(PolicyGraph)\n",
        "    def postprocess_trajectory(self,\n",
        "                               sample_batch,\n",
        "                               other_agent_batches=None,\n",
        "                               episode=None):\n",
        "        traj = {}\n",
        "        for key in sample_batch:\n",
        "            traj[key] = np.stack(sample_batch[key])\n",
        "        traj[\"cummulative_returns\"] = get_cummulative_returns(traj[SampleBatch.REWARDS], gamma= 0.99)\n",
        "        return SampleBatch(traj)\n",
        "\n",
        "\n",
        "    def get_flat_params_from(self, model):\n",
        "        params = []\n",
        "        for param in model.parameters():\n",
        "            params.append(param.data.view(-1))\n",
        "\n",
        "        flat_params = torch.cat(params)\n",
        "        return flat_params\n",
        "\n",
        "    def set_flat_params_to(self, model, flat_params):\n",
        "        prev_ind = 0\n",
        "        for param in model.parameters():\n",
        "            flat_size = int(np.prod(list(param.size())))\n",
        "            param.data.copy_(\n",
        "                flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
        "            prev_ind += flat_size\n",
        "\n",
        "    def get_loss(self, agent, observations, actions, cummulative_returns, old_probs):\n",
        "        \"\"\"\n",
        "        Computes TRPO objective\n",
        "        :param: observations - batch of observations\n",
        "        :param: actions - batch of actions\n",
        "        :param: cummulative_returns - batch of cummulative returns\n",
        "        :param: old_probs - batch of probabilities computed by old network\n",
        "        :returns: scalar value of the objective function\n",
        "        \"\"\"\n",
        "        batch_size = observations.shape[0]\n",
        "        log_probs_all = agent.get_log_probs(observations)\n",
        "        probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "        probs_for_actions = probs_all[torch.arange(\n",
        "            0, batch_size, out=torch.LongTensor()), actions]\n",
        "        old_probs_for_actions = old_probs[torch.arange(\n",
        "            0, batch_size, out=torch.LongTensor()), actions]\n",
        "\n",
        "        # Compute surrogate loss, aka importance-sampled policy gradient\n",
        "        Loss = -torch.mean(cummulative_returns * (probs_for_actions / old_probs_for_actions))\n",
        "\n",
        "        return Loss\n",
        "\n",
        "    def get_kl(self, agent, observations, actions, cummulative_returns, old_probs_all):\n",
        "        \"\"\"\n",
        "        Computes KL-divergence between network policy and old policy\n",
        "        :param: observations - batch of observations\n",
        "        :param: actions - batch of actions\n",
        "        :param: cummulative_returns - batch of cummulative returns (we don't need it actually)\n",
        "        :param: old_probs - batch of probabilities computed by old network\n",
        "        :returns: scalar value of the KL-divergence\n",
        "        \"\"\"\n",
        "        batch_size = observations.shape[0]\n",
        "        log_probs_all = agent.get_log_probs(observations)\n",
        "        probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "        # Compute Kullback-Leibler divergence (see formula above)\n",
        "        # Note: you need to sum KL and entropy over all actions, not just the ones agent took\n",
        "        old_log_probs_all = torch.log(old_probs_all + 1e-10)\n",
        "\n",
        "        kl = torch.sum(old_probs_all * (old_log_probs_all - log_probs_all)) / batch_size\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def get_entropy(self, agent, observations):\n",
        "        \"\"\"\n",
        "        Computes entropy of the network policy\n",
        "        :param: observations - batch of observations\n",
        "        :returns: scalar value of the entropy\n",
        "        \"\"\"\n",
        "\n",
        "        observations = Variable(torch.FloatTensor(observations))\n",
        "\n",
        "        batch_size = observations.shape[0]\n",
        "        log_probs_all = agent.get_log_probs(observations)\n",
        "        probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "        entropy = torch.sum(-probs_all * log_probs_all) / batch_size\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    def linesearch(self, f, x, fullstep, max_kl):\n",
        "        \"\"\"\n",
        "        Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n",
        "        :param: f - function that returns loss, kl and arbitrary third component.\n",
        "        :param: x - old parameters of neural network.\n",
        "        :param: fullstep - direction in which we make search.\n",
        "        :param: max_kl - constraint of KL divergence.\n",
        "        :returns:\n",
        "        \"\"\"\n",
        "        max_backtracks = 10\n",
        "        loss, _, = f(x)\n",
        "        for stepfrac in .5 ** np.arange(max_backtracks):\n",
        "            xnew = x + stepfrac * fullstep\n",
        "            new_loss, kl = f(xnew)\n",
        "            actual_improve = new_loss - loss\n",
        "            if kl.data.numpy() <= max_kl and actual_improve.data.numpy() < 0:\n",
        "                x = xnew\n",
        "                loss = new_loss\n",
        "        return x\n",
        "\n",
        "    def learn_on_batch(self, samples):\n",
        "        # implement your learning code here\n",
        "        max_kl = 0.01\n",
        "        observations = samples['obs']\n",
        "        actions = samples['actions']\n",
        "        returns = samples['cummulative_returns']\n",
        "        old_probs = samples['action_probs']\n",
        "        loss, kl = self.update_step(observations, actions, returns, old_probs, max_kl)\n",
        "        \n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"kl\": kl\n",
        "        }\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.get_flat_params_from(self.agent)\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.set_flat_params_to(self.agent, weights)\n",
        "        \n",
        "    def update_step(self, observations, actions, cummulative_returns, old_probs, max_kl):\n",
        "      \"\"\"\n",
        "      This function does the TRPO update step\n",
        "      :param: observations - batch of observations\n",
        "      :param: actions - batch of actions\n",
        "      :param: cummulative_returns - batch of cummulative returns\n",
        "      :param: old_probs - batch of probabilities computed by old network\n",
        "      :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n",
        "      :returns: KL between new and old policies and the value of the loss function.\n",
        "      \"\"\"\n",
        "      agent = self.agent\n",
        "\n",
        "      # Here we prepare the information\n",
        "      observations = Variable(torch.FloatTensor(observations))\n",
        "      actions = torch.LongTensor(actions)\n",
        "      cummulative_returns = Variable(torch.FloatTensor(cummulative_returns))\n",
        "      old_probs = Variable(torch.FloatTensor(old_probs))\n",
        "\n",
        "      # Here we compute gradient of the loss function\n",
        "      loss = self.get_loss(agent, observations, actions,\n",
        "                      cummulative_returns, old_probs)\n",
        "\n",
        "      grads = torch.autograd.grad(loss, agent.parameters())\n",
        "      loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
        "\n",
        "      def Fvp(v):\n",
        "          # Here we compute Fx to do solve Fx = g using conjugate gradients\n",
        "          # We actually do here a couple of tricks to compute it efficiently\n",
        "\n",
        "          kl = self.get_kl(agent, observations, actions,\n",
        "                      cummulative_returns, old_probs)\n",
        "\n",
        "          grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n",
        "          flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
        "\n",
        "          kl_v = (flat_grad_kl * Variable(v)).sum()\n",
        "         \n",
        "          grads = torch.autograd.grad(kl_v, agent.parameters())\n",
        "          flat_grad_grad_kl = torch.cat(\n",
        "              [grad.contiguous().view(-1) for grad in grads]).data\n",
        "\n",
        "          return flat_grad_grad_kl + v * 0.1\n",
        "\n",
        "      # Here we solveolve Fx = g system using conjugate gradients\n",
        "      stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n",
        "\n",
        "      # Here we compute the initial vector to do linear search\n",
        "      shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
        "\n",
        "      lm = torch.sqrt(shs / max_kl)\n",
        "      fullstep = stepdir / lm[0]\n",
        "\n",
        "\n",
        "      # Here we get the start point\n",
        "      prev_params = self.get_weights()\n",
        "\n",
        "      def get_loss_kl(params):\n",
        "          # Helper for linear search\n",
        "          # Set new params and return loss + kl\n",
        "          self.set_weights(params)\n",
        "          return [self.get_loss(agent, observations, actions, cummulative_returns, old_probs),\n",
        "                  self.get_kl(agent, observations, actions, cummulative_returns, old_probs)]\n",
        "\n",
        "      # Here we find our new parameters\n",
        "      new_params = self.linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n",
        "\n",
        "      return get_loss_kl(new_params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIUf03whVqos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll39xhuvJJXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ray.tune.logger import pretty_print\n",
        "from collections import OrderedDict\n",
        "\n",
        "def training_workflow(config, reporter):\n",
        "    # Setup policy and policy evaluation actors\n",
        "    env = gym.make(\"LunarLander-v2\")\n",
        "    policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
        "    workers = [\n",
        "        PolicyEvaluator.as_remote().remote(\n",
        "            lambda c: gym.make(\"LunarLander-v2\"),\n",
        "            CustomPolicy,\n",
        "            batch_steps=2500,\n",
        "            model_config= MODEL_DEFAULTS\n",
        "        )\n",
        "        for _ in range(config[\"num_workers\"])\n",
        "    ]\n",
        "\n",
        "    for it in range(config[\"num_iters\"]):\n",
        "        print(\"\\n********** Iteration %i ************\" % it)\n",
        "        # Broadcast weights to the policy evaluation workers\n",
        "        weights = ray.put({\"default_policy\": policy.get_weights()})\n",
        "        for w in workers:\n",
        "            w.set_weights.remote(weights)\n",
        "\n",
        "        # Gather a batch of samples\n",
        "        samples = SampleBatch.concat_samples(ray.get([w.sample.remote() for w in workers]))\n",
        "        \n",
        "        loss_stats =   policy.learn_on_batch(samples)\n",
        "        # Report current progress\n",
        "        \n",
        "        paths = samples.split_by_episode()\n",
        "        episode_lengths = [len(path[SampleBatch.ACTIONS]) for path in paths]\n",
        "        episode_sum_rewards = np.array([path[SampleBatch.REWARDS].sum() for path in paths])\n",
        "        \n",
        "        \n",
        "        stats = OrderedDict()\n",
        "        stats[\"Sampled steps\"] = len(samples[SampleBatch.ACTIONS])\n",
        "        stats[\"Episode Lengths\"] = episode_lengths\n",
        "        stats[\"Sum of rewards for episodes\"] = episode_sum_rewards\n",
        "        stats[\"Average sum of rewards per episode\"] = episode_sum_rewards.mean()\n",
        "        stats[\"Std of rewards per episode\"] = episode_sum_rewards.std()\n",
        "        stats[\"KL between old and new distribution\"] = loss_stats['kl'].data.numpy()\n",
        "        stats[\"Surrogate loss\"] = loss_stats['loss'].data.numpy()\n",
        "        for k, v in stats.items():\n",
        "            print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n",
        "        \n",
        "\n",
        "#         prev_t = 0\n",
        "#         paths = []\n",
        "#         counter = 0\n",
        "#         obervations, actions, rewards, action_probs, cum_returns = [], [], [], [], []\n",
        "#         for t in samples['t']:\n",
        "#           if t == 0 and prev_t != 0:\n",
        "            \n",
        "#             path = {\"observations\": np.array(obervations),\n",
        "#                     \"policy\": np.array(action_probs),\n",
        "#                     \"actions\": np.array(actions),\n",
        "#                     \"rewards\": np.array(rewards),\n",
        "#                     \"cumulative_returns\": np.array(cum_returns),\n",
        "#                     }\n",
        "#             obervations, actions, rewards, action_probs = [], [], [], []\n",
        "#             paths.append(path)\n",
        "#           else:  \n",
        "#             obervations.append(samples['obs'][counter])\n",
        "#             actions.append(samples['actions'][counter])\n",
        "#             action_probs.append(samples['action_probs'][counter])\n",
        "#             rewards.append(samples['rewards'][counter])   \n",
        "#             cum_returns.append(samples['cummulative_returns'][counter])\n",
        "#             prev_t = t\n",
        "#           counter+=1\n",
        "\n",
        "        # Improve the policy using the  batch\n",
        "\n",
        "\n",
        "        result =collect_metrics(remote_evaluators=workers)\n",
        "#         print(pretty_print(result))\n",
        "        \n",
        "  \n",
        "        reporter(**collect_metrics(remote_evaluators=workers))\n",
        "        \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds31NxLhw-xQ",
        "colab_type": "code",
        "outputId": "a91ef9e5-fbf3-4119-9e92-a3e69ed8f0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "ray.init()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-21 12:52:47,220\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-21_12-52-47_217991_127/logs.\n",
            "2019-05-21 12:52:47,338\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:44191 to respond...\n",
            "2019-05-21 12:52:47,476\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:13974 to respond...\n",
            "2019-05-21 12:52:47,482\tINFO services.py:806 -- Starting Redis shard with 2.52 GB max memory.\n",
            "2019-05-21 12:52:47,520\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-21_12-52-47_217991_127/logs.\n",
            "2019-05-21 12:52:47,526\tINFO services.py:1442 -- Starting the Plasma object store with 3.78 GB memory using /dev/shm.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2019-05-21_12-52-47_217991_127/sockets/plasma_store',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2019-05-21_12-52-47_217991_127/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:44191',\n",
              " 'session_dir': '/tmp/ray/session_2019-05-21_12-52-47_217991_127',\n",
              " 'webui_url': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XslGt--JJR1",
        "colab_type": "code",
        "outputId": "c39db874-d76c-4a1a-a3df-f0f41611bf1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16497
        }
      },
      "source": [
        "\n",
        "tune.run(\n",
        "        training_workflow,\n",
        "        resources_per_trial={\n",
        "            \"gpu\": 0,\n",
        "            \"cpu\": 1,\n",
        "            \"extra_cpu\": 1,\n",
        "        },\n",
        "        config={\n",
        "            \"num_workers\": 1,\n",
        "            \"num_iters\": 30,\n",
        "\n",
        "        })"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-21 13:08:10,913\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
            "2019-05-21 13:08:10,914\tINFO tune.py:223 -- Starting a new experiment.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.4/13.7 GB\n",
            "\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.4/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m 2019-05-21 13:08:13,305\tWARNING __init__.py:22 -- DeprecationWarning: PolicyGraph has been renamed to ray.rllib.policy.policy.Policy. This will raise an error in the future.\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m 2019-05-21 13:08:13,341\tWARNING worker.py:204 -- Calling ray.get or ray.wait in a separate thread may lead to deadlock if the main thread blocks on this thread and there are not enough resources to execute more tasks\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m 2019-05-21 13:08:13,383\tWARNING worker.py:342 -- WARNING: Falling back to serializing objects of type <class 'frozenset'> by using pickle. This may be inefficient.\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 0 ************\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,412\tWARNING __init__.py:22 -- DeprecationWarning: PolicyGraph has been renamed to ray.rllib.policy.policy.Policy. This will raise an error in the future.\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,413\tINFO policy_evaluator.py:731 -- Built policy map: {'default_policy': <__main__.CustomPolicy object at 0x7fd57c0108d0>}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,413\tINFO policy_evaluator.py:732 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fd57c001630>}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,413\tINFO policy_evaluator.py:343 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fd57c001b38>}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,424\tINFO policy_evaluator.py:437 -- Generating sample batch of size 2500\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,425\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((8,), dtype=float32, min=-0.171, max=1.421, mean=0.306)}}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,425\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,425\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((8,), dtype=float32, min=-0.171, max=1.421, mean=0.306)\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,426\tINFO sampler.py:411 -- Filtered obs: np.ndarray((8,), dtype=float32, min=-0.171, max=1.421, mean=0.306)\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,426\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                                   'info': None,\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                                   'obs': np.ndarray((8,), dtype=float32, min=-0.171, max=1.421, mean=0.306),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m /usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m   input = module(input)\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,489\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m { 'default_policy': ( [0],\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                       { 'action_probs': [ np.ndarray((4,), dtype=float32, min=0.194, max=0.35, mean=0.25)]})}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:15,558\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m { 'agent0': { 'data': { 'action_probs': np.ndarray((118, 4), dtype=float32, min=0.193, max=0.351, mean=0.25),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'actions': np.ndarray((118,), dtype=int64, min=0.0, max=3.0, mean=1.331),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'agent_index': np.ndarray((118,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'cummulative_returns': np.ndarray((118,), dtype=float64, min=-196.952, max=-100.0, mean=-166.327),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'dones': np.ndarray((118,), dtype=bool, min=0.0, max=1.0, mean=0.008),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'eps_id': np.ndarray((118,), dtype=int64, min=752097960.0, max=752097960.0, mean=752097960.0),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'infos': np.ndarray((118,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'new_obs': np.ndarray((118, 8), dtype=float32, min=-2.42, max=2.217, mean=0.306),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'obs': np.ndarray((118, 8), dtype=float32, min=-1.712, max=2.217, mean=0.308),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'prev_actions': np.ndarray((118,), dtype=int64, min=0.0, max=3.0, mean=1.322),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'prev_rewards': np.ndarray((118,), dtype=float32, min=-6.724, max=3.932, mean=-1.914),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'rewards': np.ndarray((118,), dtype=float32, min=-100.0, max=3.932, mean=-2.762),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         't': np.ndarray((118,), dtype=int64, min=0.0, max=117.0, mean=58.5),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m                         'unroll_id': np.ndarray((118,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m               'type': 'SampleBatch'}}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m 2019-05-21 13:08:16,943\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m { 'data': { 'action_probs': np.ndarray((2500, 4), dtype=float32, min=0.126, max=0.365, mean=0.25),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'actions': np.ndarray((2500,), dtype=int64, min=0.0, max=3.0, mean=1.373),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'agent_index': np.ndarray((2500,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'cummulative_returns': np.ndarray((2500,), dtype=float32, min=-363.414, max=1.876, mean=-146.077),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'dones': np.ndarray((2500,), dtype=bool, min=0.0, max=1.0, mean=0.011),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'eps_id': np.ndarray((2500,), dtype=int64, min=12812515.0, max=1997858321.0, mean=1181019905.08),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'infos': np.ndarray((2500,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'new_obs': np.ndarray((2500, 8), dtype=float32, min=-6.767, max=6.487, mean=0.11),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'obs': np.ndarray((2500, 8), dtype=float32, min=-2.101, max=5.03, mean=0.111),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'prev_actions': np.ndarray((2500,), dtype=int64, min=0.0, max=3.0, mean=1.355),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'prev_rewards': np.ndarray((2500,), dtype=float32, min=-29.069, max=76.321, mean=-1.898),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'rewards': np.ndarray((2500,), dtype=float32, min=-100.0, max=76.321, mean=-3.017),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             't': np.ndarray((2500,), dtype=int64, min=0.0, max=117.0, mean=44.724),\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m             'unroll_id': np.ndarray((2500,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m   'type': 'SampleBatch'}\n",
            "\u001b[2m\u001b[36m(pid=13486)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m /usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   input = module(input)\n",
            "Result for training_workflow_0:\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [118, 60, 97, 84, 117, 61, 103, 76, 108, 67, 95, 67, 109, 75, 82, 85, 108, 93, 96, 99, 86, 84, 84, 108, 66, 74, 59, 99, 40]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-325.88614  -106.31056  -615.27356  -145.77283  -406.30133  -108.34504\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -507.83066   -50.681072 -157.5548    -92.098465 -189.9906   -218.21257\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -488.76743  -141.35867  -207.39645  -435.8257   -550.41187  -359.19293\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -361.19232  -127.30365  -202.33618  -195.75974  -418.83282  -294.44543\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -112.746056 -402.64505  -139.19498  -158.54877   -21.715372]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -260.0666\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               159.94414\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009996453\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           143.4794\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-08-17\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 3.834089994430542\n",
            "  time_this_iter_s: 3.834089994430542\n",
            "  time_total_s: 3.834089994430542\n",
            "  timestamp: 1558444097\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 3 s, 1 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m /content/ray-distr/python/ray/pyarrow_files/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   out=out, **kwargs)\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m /content/ray-distr/python/ray/pyarrow_files/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 1 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [28, 94, 106, 72, 79, 87, 106, 93, 114, 107, 86, 68, 97, 81, 81, 84, 69, 92, 93, 77, 73, 61, 95, 93, 88, 57, 89, 83, 84, 63]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ 6.17980957e-02 -1.09322784e+02 -1.87233078e+02 -1.11177956e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -1.35542404e+02 -1.77916870e+02 -3.09462006e+02 -2.44228485e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -2.21424713e+02 -4.58067139e+02 -3.08086151e+02 -1.03849815e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -9.80606155e+01 -1.33321472e+02 -3.21213074e+02 -1.17191582e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -6.33347816e+01 -9.70545731e+01 -2.33099854e+02 -8.53401337e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -8.99319763e+01 -8.21601334e+01 -1.11963455e+02 -9.01297455e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -9.92912674e+00 -1.03612846e+02 -8.87388611e+01 -2.13315964e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -3.70840057e+02  2.69718666e+01]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -154.95052\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               110.51185\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009985478\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           88.7891\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 2 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [1, 91, 79, 64, 79, 59, 62, 65, 118, 71, 67, 112, 113, 85, 75, 88, 62, 77, 107, 113, 130, 64, 83, 59, 79, 89, 94, 70, 70, 94, 80]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-100.       -150.16     -131.94678   -98.19006  -176.91696   -90.8324\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -88.50433  -181.5906   -319.78894   -94.108765  -96.56794  -119.86687\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -155.28677  -130.58012    -8.130257 -303.6096    -96.29351  -151.12822\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -352.69678  -172.16632  -129.54366  -122.5306   -240.7167   -107.26387\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -122.92974  -143.08588  -373.03046  -231.35002   -55.00925  -105.71877\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -106.71692 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -153.42776\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               84.18193\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009995889\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           89.00863\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 3 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [34, 106, 97, 93, 85, 61, 76, 117, 64, 76, 81, 60, 99, 110, 83, 89, 135, 112, 66, 67, 89, 77, 68, 66, 61, 112, 95, 69, 79, 68, 5]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-1.22511871e+02 -1.64652649e+02 -2.77985657e+02 -4.90141327e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -9.77456055e+01 -1.33194199e+02 -3.20273071e+02 -8.71238403e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   2.38019714e+01 -1.50327240e+02 -1.91172195e+02 -7.95074005e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -2.79831970e+02 -1.32395813e+02 -1.43146591e+02 -1.63202591e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -1.92417282e+02 -1.01032799e+02 -9.49426498e+01 -1.59430115e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -1.54641220e+02 -1.26384773e+02 -7.22451172e+01 -6.87161713e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -1.07589622e+02 -1.57928497e+02 -1.36369904e+02 -1.08178604e+02\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -1.93603119e+02 -6.90246887e+01 -2.11372733e-01]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -146.84279\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               94.73165\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009993316\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           86.61593\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 4 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-08-23\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 10.024458169937134\n",
            "  time_this_iter_s: 1.5612537860870361\n",
            "  time_total_s: 10.024458169937134\n",
            "  timestamp: 1558444103\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 10 s, 5 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [58, 115, 118, 77, 70, 64, 133, 81, 100, 60, 110, 75, 111, 61, 72, 67, 67, 98, 84, 58, 85, 87, 89, 63, 98, 102, 57, 112, 71, 57]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -64.003685 -104.24904  -346.30212  -112.06969  -113.42986  -123.29317\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -95.51891  -111.34021   -56.748936 -120.226204 -135.87152   -90.76247\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -138.4478    -89.22911  -106.33551   -35.648834 -116.755005 -123.79558\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -115.03027   -70.791504 -137.5299   -125.82088   -11.585861  -60.165802\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -101.09906  -303.3334    -97.41769  -208.53622   -72.7612    -40.577267]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -114.28922\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               67.60379\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009990248\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           66.37913\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 5 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [38, 73, 72, 76, 72, 113, 103, 117, 122, 92, 104, 63, 119, 81, 80, 60, 92, 83, 70, 92, 124, 70, 102, 65, 119, 95, 84, 100, 19]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [   0.5524292  -94.700615  -104.66412    -94.46193    -83.17611\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -269.43048   -140.46342   -133.915     -178.82468   -335.59106\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -202.08997   -101.50958   -135.08688   -134.84805   -108.176544\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -115.83466   -168.11171   -118.08976    -62.795826  -223.78516\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -168.16275    -96.391754  -110.401924   -66.72786    -68.44534\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -73.77768    -89.63182   -189.22842    -28.606796 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -127.46129\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               68.83275\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009995698\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           73.76179\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 6 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [46, 125, 65, 64, 62, 109, 109, 70, 93, 89, 71, 81, 84, 91, 86, 89, 79, 65, 93, 108, 102, 77, 88, 59, 88, 80, 111, 72, 60, 84]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -94.02788   -400.78082   -103.77554   -109.940926  -122.217674\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -122.60412    -98.282      -79.78617    -89.89516   -109.5979\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -48.07418    -85.87354   -150.18425   -143.54556    -96.61464\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -126.494865  -136.94211    -94.814674  -135.34879   -129.60178\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -118.12814   -194.90724   -122.28644   -101.1293    -166.45114\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -108.37421   -137.60612   -134.80333    -87.1208      -3.1689563]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -121.74595\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               62.201412\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009991672\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           70.48304\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 7 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [8, 99, 88, 111, 86, 97, 94, 75, 96, 139, 100, 104, 79, 90, 69, 87, 132, 122, 93, 117, 95, 79, 68, 82, 117, 113, 60]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -91.11924   -98.34457    11.56739   -98.493484   38.622894 -119.802795\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -137.40134   -34.186356  -83.92933  -133.94241  -138.03934  -136.00757\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -113.61787  -143.10333   -95.08494    21.628716    4.563938 -101.34736\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -99.64332   -87.17272   -85.24979  -152.38647   -77.19288  -194.09024\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -80.150826  -80.34415   -24.327505]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -86.244255\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               55.683537\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009985135\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           46.069077\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 8 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-08-29\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 16.29514789581299\n",
            "  time_this_iter_s: 1.614880084991455\n",
            "  time_total_s: 16.29514789581299\n",
            "  timestamp: 1558444109\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 16 s, 9 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [75, 83, 113, 89, 182, 86, 136, 116, 134, 87, 100, 78, 121, 118, 87, 81, 131, 121, 73, 118, 64, 76, 90, 101, 40]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -52.029114 -131.00348  -181.85284   -80.28616   -63.919964  -62.839386\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -54.45233  -136.82382   -36.402077  -80.78284  -108.678314  -58.384193\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -109.477875  -22.713188  -31.622383 -114.25903    28.79649  -121.45275\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -98.43716   -82.98459   -84.2428   -231.9969   -107.24311  -125.68603\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -29.045385]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -87.11277\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               53.003494\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009993319\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           45.349106\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 9 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [68, 116, 110, 77, 123, 71, 88, 81, 149, 78, 83, 109, 109, 119, 67, 93, 102, 64, 101, 83, 135, 59, 84, 78, 87, 133, 33]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -29.112389   -97.14816    -94.07393    -76.62001    -85.1777\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -79.12525   -100.356865   -88.20566     -4.6393757 -108.3577\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -66.29437   -156.42126    -81.78711   -134.0743     -54.5628\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -84.36727    -55.68372    -50.57276   -105.34609   -102.70464\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -88.30443    -92.690575   -68.4156     -59.32608    -47.982178\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -76.76887     -8.575466 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -77.65536\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               32.830997\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009976284\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           46.867687\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 10 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [40, 80, 126, 139, 126, 136, 76, 119, 96, 81, 77, 93, 107, 125, 77, 138, 112, 114, 88, 108, 65, 151, 124, 102]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -64.030266  -81.60537  -105.79959  -311.10965   -62.827415  -31.083307\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -26.23793   -54.14213   -64.71629  -193.1659    -49.478584  -48.604492\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -146.64719  -204.07297   -77.58946   -69.30587   -86.198906  -48.24393\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -100.247314 -169.22937   -84.04053  -115.25218   -34.366646  -24.938692]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -93.87225\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               66.66723\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009983996\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           52.764206\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 11 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [33, 86, 93, 85, 139, 118, 110, 107, 63, 109, 75, 101, 109, 73, 95, 81, 139, 126, 97, 102, 70, 90, 139, 140, 120]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -56.60514     -3.728096   -60.604572   -75.13371    -98.662704\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -57.365288   -16.012794   -71.09297    -88.66524      3.8998642\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -83.46706    -77.03488   -158.48245    -32.92987    -38.2008\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -70.36116   -122.94399   -294.19217    -90.84014    -67.579956\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -68.69949    -54.05923    -47.76573   -109.18817     37.404972 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -72.09243\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               60.68137\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009986474\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           41.31694\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 12 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-08-35\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 22.642512798309326\n",
            "  time_this_iter_s: 1.5956792831420898\n",
            "  time_total_s: 22.642512798309326\n",
            "  timestamp: 1558444115\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 13\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 22 s, 13 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [1, 92, 149, 106, 105, 106, 145, 115, 134, 144, 87, 156, 129, 139, 80, 156, 113, 90, 135, 105, 122, 91]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-100.         -40.186028   -53.870758   -76.23714    -66.72322\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -78.10173    -49.48625    -38.550545   -93.45091    -52.213898\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -48.58921   -169.6642     -77.66124    -22.683563   -61.73155\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -55.87117   -158.07294      1.8717957 -255.10298    -90.15379\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -83.013725   -46.062027 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -77.97978\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               54.0751\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009990093\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           36.830605\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 13 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [33, 90, 130, 125, 102, 63, 94, 100, 88, 63, 83, 114, 99, 134, 107, 87, 82, 144, 169, 101, 127, 89, 108, 119, 49]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [  -9.134125   -94.53947   -106.19263    -19.313423   -76.25486\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -89.23863    -45.7458      -7.367485   -39.703953   -99.575\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -85.504425   -79.180786  -121.60098    -65.437454  -110.73688\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -95.35546   -137.28969    -85.66646     -5.7678695   12.253792\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -70.25059    -24.971893  -108.05234    -77.74335      1.541828 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -65.63312\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               41.976715\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009991307\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           38.435947\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 14 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [102, 107, 148, 160, 235, 121, 79, 123, 107, 88, 110, 122, 154, 125, 138, 81, 116, 124, 97, 143, 20]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -16.192154   -15.887138   -43.440063  -134.24088   -170.7374\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -22.447876   -39.522648  -109.359566   -25.949432  -105.83814\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -71.77678   -181.22476     -6.2899256  -14.987892    -2.3934526\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -30.155449  -110.437935   -27.720276   -67.77203   -184.23764\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -16.74779  ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -66.54092\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               59.1399\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009981083\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           37.666073\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 15 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-08-42\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 29.4925594329834\n",
            "  time_this_iter_s: 3.2698748111724854\n",
            "  time_total_s: 29.4925594329834\n",
            "  timestamp: 1558444122\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 16\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 29 s, 16 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [102, 96, 148, 1000, 136, 160, 174, 99, 144, 161, 153, 98, 29]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-4.53195496e+01 -1.06782364e+02 -1.21628571e+01  2.54917984e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -7.61251984e+01 -3.29633270e+02  1.20254307e+01 -9.18247757e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -5.99965706e+01  3.46926537e+01  2.89517231e+01 -2.32969360e+01\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -2.88086891e-01]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -49.559082\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               92.62767\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009986363\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           19.408028\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 16 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [91, 176, 167, 121, 116, 194, 138, 140, 1000, 77, 173, 107]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [  25.067001  -207.80318    -51.286636    -5.3879395  -81.688774\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -58.26848    -64.18822    -40.858826    21.78553     11.0132065\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -207.4695      34.874912 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -52.017574\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               78.90581\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009980845\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           20.671\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 17 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [24, 176, 128, 126, 91, 139, 141, 135, 127, 111, 165, 132, 134, 103, 133, 143, 143, 146, 103, 100]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -23.11975     31.773588   -41.768425   -20.795555   -48.20215\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -75.68976    -36.23845    -22.164524    -7.2148743   44.328903\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -49.204235     2.9571395   -1.9818716 -226.66032    -21.679852\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -36.574623   -20.871815   -23.230532   -28.098267    46.760067 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -27.883764\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               54.59519\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009990396\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           13.224338\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 18 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-08-50\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 37.597548723220825\n",
            "  time_this_iter_s: 3.175381898880005\n",
            "  time_total_s: 37.597548723220825\n",
            "  timestamp: 1558444130\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 19\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 37 s, 19 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [27, 175, 283, 108, 166, 148, 1000, 89, 136, 118, 144, 106]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -70.785095  -98.90282  -149.6772    -60.99873   -51.536392  -32.785923\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m    19.570297  -43.74595   -27.89139   -20.140442   27.204535   36.9257  ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -39.39695\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               51.320683\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009992203\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           16.642988\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 19 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [63, 106, 91, 173, 1000, 1000, 67]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-24.794525  14.298927 -25.735306  16.559193  54.77053  -48.74218\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   19.13047 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       0.7838729\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               32.71746\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009974321\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           1.3333113\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 20 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-09-00\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 46.827516078948975\n",
            "  time_this_iter_s: 4.3457252979278564\n",
            "  time_total_s: 46.827516078948975\n",
            "  timestamp: 1558444140\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 21\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 46 s, 21 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [933, 199, 173, 160, 98, 151, 786]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -26.718811   21.141823  -55.777428   16.444675 -129.48106    29.904533\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m    40.974106]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -14.787452\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               56.50568\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.0099895885\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           6.4627037\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 21 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [214, 205, 153, 133, 219, 426, 1000, 111, 39]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -35.807953  -63.168327 -148.2893    -85.82012  -222.47812  -134.6851\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m    47.170357  -28.883171   12.197789]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -73.307106\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               79.65658\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009976875\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           26.934887\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 22 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [961, 195, 1000, 344]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -5.7044506 -10.434872   59.78437   131.25336  ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       43.7246\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               57.653446\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009996324\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           -2.9322598\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-09-08\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 55.50214982032776\n",
            "  time_this_iter_s: 4.645549774169922\n",
            "  time_total_s: 55.50214982032776\n",
            "  timestamp: 1558444148\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 23\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 55 s, 23 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 23 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [656, 176, 212, 93, 132, 152, 264, 168, 647]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-106.722435   28.757225 -148.09921  -194.4711     28.851307  -23.689386\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m  -170.09862   -57.126595   67.56289 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -63.892883\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               90.29806\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009996784\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           20.657919\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 24 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-09-16\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 63.70152187347412\n",
            "  time_this_iter_s: 4.751072883605957\n",
            "  time_total_s: 63.70152187347412\n",
            "  timestamp: 1558444156\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 25\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 63 s, 25 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [353, 191, 1000, 233, 723]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -79.12869  -207.91696    68.927444 -111.44046    73.24501 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -51.262733\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               108.52306\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009969694\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           11.849203\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 25 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [277, 119, 140, 616, 133, 1000, 134, 81]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -40.64767   -35.018463  -24.221384 -204.00696   -82.997765 -125.96275\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -58.92281   102.691055]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -58.63584\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               82.425224\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009974193\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           19.998646\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 26 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-09-25\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 72.5225613117218\n",
            "  time_this_iter_s: 4.536881685256958\n",
            "  time_total_s: 72.5225613117218\n",
            "  timestamp: 1558444165\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 27\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 72 s, 27 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [919, 95, 103, 122, 1000, 261]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-101.728775     6.301628    -3.5976715    9.929512   -41.746384\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m    95.421745 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -5.903324\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               59.356224\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009974955\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           5.854264\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 27 ************\n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-09-31\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 77.91225934028625\n",
            "  time_this_iter_s: 5.389698028564453\n",
            "  time_total_s: 77.91225934028625\n",
            "  timestamp: 1558444171\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 28\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 77 s, 28 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [739, 1000, 142, 131, 488]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [-59.96238    18.428467    9.164039    7.1410522  28.945473 ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       0.74333036\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               31.32139\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.009992868\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           2.1044188\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 28 ************\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [512, 172, 189, 141, 204, 90, 353, 140, 160, 539]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -30.70134    -34.967514    25.277561     4.079262   -22.55569\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m   -29.984993  -255.12079      9.950901    -4.1945763  -53.02849  ]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -39.124565\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               75.456406\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.00999647\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           20.07497\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m ********** Iteration 29 ************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-21 13:09:38,447\tINFO ray_trial_executor.py:180 -- Destroying actor for trial training_workflow_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-21_13-09-38\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 3d32ff6e1b054241ba3a5649a18bd3a1\n",
            "  hostname: e929e001e077\n",
            "  iterations_since_restore: 30\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 13482\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 85.14796900749207\n",
            "  time_this_iter_s: 4.312246561050415\n",
            "  time_total_s: 85.14796900749207\n",
            "  timestamp: 1558444178\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 30\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=13482], 85 s, 30 iter, nan rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sampled steps:                            2500\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Episode Lengths:                          [461, 160, 174, 1000, 461, 244]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Sum of rewards for episodes:              [ -26.057056  -60.53735  -260.07184    11.248322 -309.91772    59.255005]\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Average sum of rewards per episode:       -97.680115\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Std of rewards per episode:               138.09378\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m KL between old and new distribution:      0.0099977385\n",
            "\u001b[2m\u001b[36m(pid=13482)\u001b[0m Surrogate loss:                           24.511074\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 2.7/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'TERMINATED': 1})\n",
            "TERMINATED trials:\n",
            " - training_workflow_0:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=13482], 85 s, 30 iter, nan rew\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[training_workflow_0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k7DmyvCI_WJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trpo-ray.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denklewer/ray-custom-agents/blob/master/trpo_ray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAGWDBl0Cxk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install -y build-essential curl unzip psmisc\n",
        "!pip install cython==0.29.0\n",
        "!git clone https://github.com/ray-project/ray.git\n",
        "!ray/ci/travis/install-bazel.sh\n",
        "!pip install lz4\n",
        "!pip install setproctitle\n",
        "!mv ray ray-distr\n",
        "!pip install -e ray-distr/python/. --verbose  # Add --user if you see a permission denied error."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSOf6g3sfOP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(\"\", '/content/ray-distr/python')) # To find local version of the library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_iCFvpCJJLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import gym\n",
        "import logging\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.evaluation import PolicyGraph, PolicyEvaluator, SampleBatch\n",
        "from ray.rllib.evaluation.metrics import collect_metrics\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "import scipy.signal\n",
        "from ray.rllib.utils.annotations import override\n",
        "logger = logging.getLogger(__name__)\n",
        "from ray.rllib.evaluation.postprocessing import compute_advantages,Postprocessing\n",
        "\n",
        "from torch.autograd import Variable\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--gpu\", action=\"store_true\")\n",
        "parser.add_argument(\"--num-iters\", type=int, default=20)\n",
        "parser.add_argument(\"--num-workers\", type=int, default=2)\n",
        "\n",
        "class TRPOAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, hidden_size=32):\n",
        "        '''\n",
        "        Here you should define your model\n",
        "        You should have LOG-PROBABILITIES as output because you will need it to compute loss\n",
        "        We recommend that you start simple:\n",
        "        use 1-2 hidden layers with 100-500 units and relu for the first try\n",
        "        '''\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.n_actions = n_actions\n",
        "        self.state_hape = state_shape\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_shape[0], hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"\n",
        "        takes agent's observation (Variable), returns log-probabilities (Variable)\n",
        "        :param state_t: a batch of states, shape = [batch_size, state_shape]\n",
        "        \"\"\"\n",
        "\n",
        "        # Use your network to compute log_probs for given state\n",
        "        log_probs = self.model(states)\n",
        "        return log_probs\n",
        "\n",
        "    def get_log_probs(self, states):\n",
        "        '''\n",
        "        Log-probs for training\n",
        "        '''\n",
        "\n",
        "        return self.forward(states)\n",
        "\n",
        "    def get_probs(self, states):\n",
        "        '''\n",
        "        Probs for interaction\n",
        "        '''\n",
        "\n",
        "        return torch.exp(self.forward(states))\n",
        "\n",
        "    def act(self, obs, sample=True):\n",
        "        '''\n",
        "        Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n",
        "        :param: obs - single observation vector\n",
        "        :param sample: if True, samples from \\pi, otherwise takes most likely action\n",
        "        :returns: action (single integer) and probabilities for all actions\n",
        "        '''\n",
        "\n",
        "        probs = self.get_probs(Variable(torch.FloatTensor([obs]))).data.numpy()\n",
        "\n",
        "        if sample:\n",
        "            action = int(np.random.choice(self.n_actions, p=probs[0]))\n",
        "        else:\n",
        "            action = int(np.argmax(probs))\n",
        "\n",
        "        return action, probs[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qxmLg0Gdmqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UTIL FUNCTIONS\n",
        "def get_cummulative_returns(r, gamma=1):\n",
        "  \"\"\"\n",
        "  Computes cummulative discounted rewards given immediate rewards\n",
        "        G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
        "        Also known as R(s,a)\n",
        "  \"\"\"\n",
        "  r = np.array(r)\n",
        "  assert r.ndim >= 1\n",
        "  return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]\n",
        "\n",
        "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
        "    \"\"\"\n",
        "    This method solves system of equation Ax=b using iterative method called conjugate gradients\n",
        "    :f_Ax: function that returns Ax\n",
        "    :b: targets for Ax\n",
        "    :cg_iters: how many iterations this method should do\n",
        "    :residual_tol: epsilon for stability\n",
        "    \"\"\"\n",
        "    p = b.clone()\n",
        "    r = b.clone()\n",
        "    x = torch.zeros(b.size())\n",
        "    rdotr = torch.sum(r * r)\n",
        "    for i in range(cg_iters):\n",
        "        z = f_Ax(p)\n",
        "        v = rdotr / (torch.sum(p * z) + 1e-8)\n",
        "        x += v * p\n",
        "        r -= v * z\n",
        "        newrdotr = torch.sum(r * r)\n",
        "        mu = newrdotr / (rdotr + 1e-8)\n",
        "        p = r + mu * p\n",
        "        rdotr = newrdotr\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioP5xoJbJJZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomPolicy(PolicyGraph):\n",
        "    \"\"\"Example of a custom policy graph written from scratch.\n",
        "    You might find it more convenient to extend TF/TorchPolicyGraph instead\n",
        "    for a real policy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space, action_space, config):\n",
        "        PolicyGraph.__init__(self, observation_space, action_space, config)\n",
        "        # example parameter\n",
        "        self.w = 1.0\n",
        "        self.observation_shape = observation_space.shape\n",
        "        self.n_actions = action_space.n\n",
        "        self.agent = TRPOAgent(self.observation_shape, self.n_actions)\n",
        "        self.policy = []\n",
        "\n",
        "\n",
        "    def compute_actions(self,\n",
        "                        obs_batch,\n",
        "                        state_batches,\n",
        "                        prev_action_batch=None,\n",
        "                        prev_reward_batch=None,\n",
        "                        info_batch=None,\n",
        "                        episodes=None,\n",
        "                        **kwargs):\n",
        "        # return random actions\n",
        "        actions = []\n",
        "        action_probs = []\n",
        "        for obs in obs_batch:\n",
        "            action, policy = self.agent.act(obs)\n",
        "            actions.append(action)\n",
        "            action_probs.append(policy)\n",
        "\n",
        "        return actions, [], {\"action_probs\": action_probs}\n",
        "      \n",
        "    @override(PolicyGraph)\n",
        "    def postprocess_trajectory(self,\n",
        "                               sample_batch,\n",
        "                               other_agent_batches=None,\n",
        "                               episode=None):\n",
        "        traj = {}\n",
        "        for key in sample_batch:\n",
        "            traj[key] = np.stack(sample_batch[key])\n",
        "        traj[\"cummulative_returns\"] = get_cummulative_returns(traj[SampleBatch.REWARDS])\n",
        "        return SampleBatch(traj)\n",
        "\n",
        "\n",
        "    def get_flat_params_from(self, model):\n",
        "        params = []\n",
        "        for param in model.parameters():\n",
        "            params.append(param.data.view(-1))\n",
        "\n",
        "        flat_params = torch.cat(params)\n",
        "        return flat_params\n",
        "\n",
        "    def set_flat_params_to(self, model, flat_params):\n",
        "        prev_ind = 0\n",
        "        for param in model.parameters():\n",
        "            flat_size = int(np.prod(list(param.size())))\n",
        "            param.data.copy_(\n",
        "                flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
        "            prev_ind += flat_size\n",
        "\n",
        "    def get_loss(self, agent, observations, actions, cummulative_returns, old_probs):\n",
        "        \"\"\"\n",
        "        Computes TRPO objective\n",
        "        :param: observations - batch of observations\n",
        "        :param: actions - batch of actions\n",
        "        :param: cummulative_returns - batch of cummulative returns\n",
        "        :param: old_probs - batch of probabilities computed by old network\n",
        "        :returns: scalar value of the objective function\n",
        "        \"\"\"\n",
        "        batch_size = observations.shape[0]\n",
        "        log_probs_all = agent.get_log_probs(observations)\n",
        "        probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "        probs_for_actions = probs_all[torch.arange(\n",
        "            0, batch_size, out=torch.LongTensor()), actions]\n",
        "        old_probs_for_actions = old_probs[torch.arange(\n",
        "            0, batch_size, out=torch.LongTensor()), actions]\n",
        "\n",
        "        # Compute surrogate loss, aka importance-sampled policy gradient\n",
        "        Loss = -torch.mean(cummulative_returns * (probs_for_actions / old_probs_for_actions))\n",
        "\n",
        "        return Loss\n",
        "\n",
        "    def get_kl(self, agent, observations, actions, cummulative_returns, old_probs_all):\n",
        "        \"\"\"\n",
        "        Computes KL-divergence between network policy and old policy\n",
        "        :param: observations - batch of observations\n",
        "        :param: actions - batch of actions\n",
        "        :param: cummulative_returns - batch of cummulative returns (we don't need it actually)\n",
        "        :param: old_probs - batch of probabilities computed by old network\n",
        "        :returns: scalar value of the KL-divergence\n",
        "        \"\"\"\n",
        "        batch_size = observations.shape[0]\n",
        "        log_probs_all = agent.get_log_probs(observations)\n",
        "        probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "        # Compute Kullback-Leibler divergence (see formula above)\n",
        "        # Note: you need to sum KL and entropy over all actions, not just the ones agent took\n",
        "        old_log_probs_all = torch.log(old_probs_all + 1e-10)\n",
        "\n",
        "        kl = torch.sum(old_probs_all * (old_log_probs_all - log_probs_all)) / batch_size\n",
        "\n",
        "        return kl\n",
        "\n",
        "    def get_entropy(self, agent, observations):\n",
        "        \"\"\"\n",
        "        Computes entropy of the network policy\n",
        "        :param: observations - batch of observations\n",
        "        :returns: scalar value of the entropy\n",
        "        \"\"\"\n",
        "\n",
        "        observations = Variable(torch.FloatTensor(observations))\n",
        "\n",
        "        batch_size = observations.shape[0]\n",
        "        log_probs_all = agent.get_log_probs(observations)\n",
        "        probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "        entropy = torch.sum(-probs_all * log_probs_all) / batch_size\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    def linesearch(self, f, x, fullstep, max_kl):\n",
        "        \"\"\"\n",
        "        Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n",
        "        :param: f - function that returns loss, kl and arbitrary third component.\n",
        "        :param: x - old parameters of neural network.\n",
        "        :param: fullstep - direction in which we make search.\n",
        "        :param: max_kl - constraint of KL divergence.\n",
        "        :returns:\n",
        "        \"\"\"\n",
        "        max_backtracks = 10\n",
        "        loss, _, = f(x)\n",
        "        for stepfrac in .5 ** np.arange(max_backtracks):\n",
        "            xnew = x + stepfrac * fullstep\n",
        "            new_loss, kl = f(xnew)\n",
        "            actual_improve = new_loss - loss\n",
        "            if kl.data.numpy() <= max_kl and actual_improve.data.numpy() < 0:\n",
        "                x = xnew\n",
        "                loss = new_loss\n",
        "        return x\n",
        "\n",
        "    def learn_on_batch(self, samples):\n",
        "        # implement your learning code here\n",
        "        max_kl = 0.01\n",
        "        observations = samples['obs']\n",
        "        actions = samples['actions']\n",
        "        returns = samples['cummulative_returns']\n",
        "        old_probs = samples['action_probs']\n",
        "        loss, kl = self.update_step(observations, actions, returns, old_probs, max_kl)\n",
        "        \n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"kl\": kl\n",
        "        }\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.get_flat_params_from(self.agent)\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.set_flat_params_to(self.agent, weights)\n",
        "        \n",
        "    def update_step(self, observations, actions, cummulative_returns, old_probs, max_kl):\n",
        "      \"\"\"\n",
        "      This function does the TRPO update step\n",
        "      :param: observations - batch of observations\n",
        "      :param: actions - batch of actions\n",
        "      :param: cummulative_returns - batch of cummulative returns\n",
        "      :param: old_probs - batch of probabilities computed by old network\n",
        "      :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n",
        "      :returns: KL between new and old policies and the value of the loss function.\n",
        "      \"\"\"\n",
        "      agent = self.agent\n",
        "\n",
        "      # Here we prepare the information\n",
        "      observations = Variable(torch.FloatTensor(observations))\n",
        "      actions = torch.LongTensor(actions)\n",
        "      cummulative_returns = Variable(torch.FloatTensor(cummulative_returns))\n",
        "      old_probs = Variable(torch.FloatTensor(old_probs))\n",
        "\n",
        "      # Here we compute gradient of the loss function\n",
        "      loss = self.get_loss(agent, observations, actions,\n",
        "                      cummulative_returns, old_probs)\n",
        "      grads = torch.autograd.grad(loss, agent.parameters())\n",
        "      loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
        "\n",
        "      def Fvp(v):\n",
        "          # Here we compute Fx to do solve Fx = g using conjugate gradients\n",
        "          # We actually do here a couple of tricks to compute it efficiently\n",
        "\n",
        "          kl = self.get_kl(agent, observations, actions,\n",
        "                      cummulative_returns, old_probs)\n",
        "\n",
        "          grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n",
        "          flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
        "\n",
        "          kl_v = (flat_grad_kl * Variable(v)).sum()\n",
        "          grads = torch.autograd.grad(kl_v, agent.parameters())\n",
        "          flat_grad_grad_kl = torch.cat(\n",
        "              [grad.contiguous().view(-1) for grad in grads]).data\n",
        "\n",
        "          return flat_grad_grad_kl + v * 0.1\n",
        "\n",
        "      # Here we solveolve Fx = g system using conjugate gradients\n",
        "      stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n",
        "\n",
        "      # Here we compute the initial vector to do linear search\n",
        "      shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
        "\n",
        "      lm = torch.sqrt(shs / max_kl)\n",
        "      fullstep = stepdir / lm[0]\n",
        "\n",
        "\n",
        "      # Here we get the start point\n",
        "      prev_params = self.get_weights()\n",
        "\n",
        "      def get_loss_kl(params):\n",
        "          # Helper for linear search\n",
        "          # Set new params and return loss + kl\n",
        "          self.set_weights(params)\n",
        "          return [self.get_loss(agent, observations, actions, cummulative_returns, old_probs),\n",
        "                  self.get_kl(agent, observations, actions, cummulative_returns, old_probs)]\n",
        "\n",
        "      # Here we find our new parameters\n",
        "      new_params = self.linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n",
        "\n",
        "      return get_loss_kl(new_params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIUf03whVqos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll39xhuvJJXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ray.tune.logger import pretty_print\n",
        "from collections import OrderedDict\n",
        "\n",
        "def training_workflow(config, reporter):\n",
        "    # Setup policy and policy evaluation actors\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
        "    workers = [\n",
        "        PolicyEvaluator.as_remote().remote(lambda c: gym.make(\"CartPole-v0\"),\n",
        "                                           CustomPolicy)\n",
        "        for _ in range(config[\"num_workers\"])\n",
        "    ]\n",
        "\n",
        "    for it in range(config[\"num_iters\"]):\n",
        "        print(\"\\n********** Iteration %i ************\" % it)\n",
        "        # Broadcast weights to the policy evaluation workers\n",
        "        weights = ray.put({\"default_policy\": policy.get_weights()})\n",
        "        for w in workers:\n",
        "            w.set_weights.remote(weights)\n",
        "\n",
        "        # Gather a batch of samples\n",
        "        samples = SampleBatch.concat_samples(ray.get([w.sample.remote() for w in workers]))\n",
        "\n",
        "\n",
        "        prev_t = 0\n",
        "        paths = []\n",
        "        counter = 0\n",
        "        obervations, actions, rewards, action_probs, cum_returns = [], [], [], [], []\n",
        "        for t in samples['t']:\n",
        "          if t == 0 and prev_t != 0:\n",
        "            \n",
        "            path = {\"observations\": np.array(obervations),\n",
        "                    \"policy\": np.array(action_probs),\n",
        "                    \"actions\": np.array(actions),\n",
        "                    \"rewards\": np.array(rewards),\n",
        "                    \"cumulative_returns\": np.array(cum_returns),\n",
        "                    }\n",
        "            obervations, actions, rewards, action_probs = [], [], [], []\n",
        "            paths.append(path)\n",
        "          else:  \n",
        "            obervations.append(samples['obs'][counter])\n",
        "            actions.append(samples['actions'][counter])\n",
        "            action_probs.append(samples['action_probs'][counter])\n",
        "            rewards.append(samples['rewards'][counter])   \n",
        "            cum_returns.append(samples['cummulative_returns'][counter])\n",
        "            prev_t = t\n",
        "          counter+=1\n",
        "\n",
        "        # Improve the policy using the  batch\n",
        "\n",
        "\n",
        "        loss_stats =   policy.learn_on_batch(samples)\n",
        "        # Report current progress\n",
        "        result =collect_metrics(remote_evaluators=workers)\n",
        "#         print(pretty_print(result))\n",
        "        \n",
        "        \n",
        "        \n",
        "            # Report current progress\n",
        "        episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n",
        "\n",
        "        stats = OrderedDict()\n",
        "        stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n",
        "        stats[\"rewards\"] = episode_rewards\n",
        "        stats[\"Std of rewards per episode\"] = episode_rewards.std()\n",
        "        stats[\"KL between old and new distribution\"] = loss_stats['kl'].data.numpy()\n",
        "        stats[\"Surrogate loss\"] = loss_stats['loss'].data.numpy()\n",
        "        for k, v in stats.items():\n",
        "            print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n",
        "        reporter(**collect_metrics(remote_evaluators=workers))\n",
        "        \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds31NxLhw-xQ",
        "colab_type": "code",
        "outputId": "355a59be-6d64-415e-bdb1-ab7c3be613d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "ray.init()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-20 00:40:33,743\tWARNING worker.py:1341 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-3f68a533b944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ray-distr/python/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(redis_address, num_cpus, num_gpus, resources, object_store_memory, redis_max_memory, log_to_driver, node_ip_address, object_id_seed, local_mode, redirect_worker_output, redirect_output, ignore_reinit_error, num_redis_shards, redis_max_clients, redis_password, plasma_directory, huge_pages, include_webui, driver_id, configure_logging, logging_level, logging_format, plasma_store_socket_name, raylet_socket_name, temp_dir, load_code_from_local, _internal_config)\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m             raise Exception(\"Perhaps you called ray.init twice by accident? \"\n\u001b[0m\u001b[1;32m   1352\u001b[0m                             \u001b[0;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                             \u001b[0;34m\"'ignore_reinit_error=True' or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Perhaps you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XslGt--JJR1",
        "colab_type": "code",
        "outputId": "8880b69b-131b-44be-95fa-317aacc665e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3561
        }
      },
      "source": [
        "\n",
        "tune.run(\n",
        "        training_workflow,\n",
        "        resources_per_trial={\n",
        "            \"gpu\": 0,\n",
        "            \"cpu\": 1,\n",
        "            \"extra_cpu\": 1,\n",
        "        },\n",
        "        config={\n",
        "            \"num_workers\": 1,\n",
        "            \"num_iters\": 10,\n",
        "        })"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-20 00:42:45,544\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
            "2019-05-20 00:42:45,545\tINFO tune.py:223 -- Starting a new experiment.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 8.1/13.7 GB\n",
            "\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 8.1/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - training_workflow_0:\tRUNNING\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m 2019-05-20 00:42:49,189\tWARNING worker.py:204 -- Calling ray.get or ray.wait in a separate thread may lead to deadlock if the main thread blocks on this thread and there are not enough resources to execute more tasks\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 0 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m /usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m   input = module(input)\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,533\tINFO policy_evaluator.py:732 -- Built policy map: {'default_policy': <__main__.CustomPolicy object at 0x7fddbb239a90>}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,534\tINFO policy_evaluator.py:733 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fde9cc3f4e0>}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,534\tINFO policy_evaluator.py:344 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fde9cc80470>}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,539\tINFO policy_evaluator.py:438 -- Generating sample batch of size 100\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,539\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.048, max=0.04, mean=0.002)}}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,539\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,540\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.048, max=0.04, mean=0.002)\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,541\tINFO sampler.py:411 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.048, max=0.04, mean=0.002)\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,542\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                                   'info': None,\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.048, max=0.04, mean=0.002),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m /usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m   input = module(input)\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,544\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m { 'default_policy': ( [0],\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                       { 'action_probs': [ np.ndarray((2,), dtype=float32, min=0.451, max=0.549, mean=0.5)]})}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,554\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m { 'agent0': { 'data': { 'action_probs': np.ndarray((19, 2), dtype=float32, min=0.421, max=0.579, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'actions': np.ndarray((19,), dtype=int64, min=0.0, max=1.0, mean=0.316),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'agent_index': np.ndarray((19,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'cummulative_returns': np.ndarray((19,), dtype=float64, min=1.0, max=19.0, mean=10.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'dones': np.ndarray((19,), dtype=bool, min=0.0, max=1.0, mean=0.053),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'eps_id': np.ndarray((19,), dtype=int64, min=1829664304.0, max=1829664304.0, mean=1829664304.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'infos': np.ndarray((19,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'new_obs': np.ndarray((19, 4), dtype=float32, min=-1.425, max=2.369, mean=0.081),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'obs': np.ndarray((19, 4), dtype=float32, min=-1.228, max=2.02, mean=0.067),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'prev_actions': np.ndarray((19,), dtype=int64, min=0.0, max=1.0, mean=0.316),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'prev_rewards': np.ndarray((19,), dtype=float32, min=0.0, max=1.0, mean=0.947),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'rewards': np.ndarray((19,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         't': np.ndarray((19,), dtype=int64, min=0.0, max=18.0, mean=9.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m                         'unroll_id': np.ndarray((19,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m               'type': 'SampleBatch'}}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m 2019-05-20 00:42:49,588\tINFO policy_evaluator.py:475 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m { 'data': { 'action_probs': np.ndarray((100, 2), dtype=float32, min=0.419, max=0.581, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'actions': np.ndarray((100,), dtype=int64, min=0.0, max=1.0, mean=0.29),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'cummulative_returns': np.ndarray((100,), dtype=float32, min=1.0, max=28.0, mean=10.4),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'eps_id': np.ndarray((100,), dtype=int64, min=375659559.0, max=1829664304.0, mean=1205234968.79),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'infos': np.ndarray((100,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'new_obs': np.ndarray((100, 4), dtype=float32, min=-1.785, max=2.647, mean=0.065),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'obs': np.ndarray((100, 4), dtype=float32, min=-1.749, max=2.323, mean=0.052),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'prev_actions': np.ndarray((100,), dtype=int64, min=0.0, max=1.0, mean=0.27),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'prev_rewards': np.ndarray((100,), dtype=float32, min=0.0, max=1.0, mean=0.94),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'rewards': np.ndarray((100,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             't': np.ndarray((100,), dtype=int64, min=0.0, max=27.0, mean=9.4),\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m             'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m   'type': 'SampleBatch'}\n",
            "\u001b[2m\u001b[36m(pid=16872)\u001b[0m \n",
            "Result for training_workflow_0:\n",
            "  custom_metrics: {}\n",
            "  date: 2019-05-20_00-42-49\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 7df43796f2094b48a3e8b0a334840d6c\n",
            "  hostname: 3db777dd2d2e\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_metric_batches_dropped: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 16866\n",
            "  policy_reward_mean: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 0.4776163101196289\n",
            "  time_this_iter_s: 0.4776163101196289\n",
            "  time_total_s: 0.4776163101196289\n",
            "  timestamp: 1558312969\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m /content/ray-distr/python/ray/pyarrow_files/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m   out=out, **kwargs)\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m /content/ray-distr/python/ray/pyarrow_files/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       14.6\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [19. 11.  8. 27.  8.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               7.3918877\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009970513\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -10.696159\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 1 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       12.0\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [ 1. 10. 25. 19.  9. 10. 10.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               7.1713715\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009987009\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -8.950245\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 2 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       15.333333\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [ 8. 12. 13. 11. 39.  9.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               10.718623\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009989743\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -12.350677\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 3 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       24.0\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [23. 23. 24. 26.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               1.2247449\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009983121\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -13.158063\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 4 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       18.4\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [19. 15. 26. 13. 19.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               4.4542117\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009972792\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -10.629867\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 5 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       24.0\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [29. 31. 12.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               8.524474\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.0099747535\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -14.361095\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 6 ************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-20 00:42:50,488\tINFO ray_trial_executor.py:180 -- Destroying actor for trial training_workflow_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       22.75\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [ 6. 31. 23. 31.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               10.207228\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009984713\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -14.456856\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 7 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       39.0\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [55. 23.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               16.0\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009987923\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -21.609226\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 8 ************\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Average sum of rewards per episode:       33.5\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m rewards:                                  [37. 30.]\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Std of rewards per episode:               3.5\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m KL between old and new distribution:      0.009973451\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m Surrogate loss:                           -17.815903\n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=16866)\u001b[0m ********** Iteration 9 ************\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs\n",
            "Memory usage on this node: 8.4/13.7 GB\n",
            "Result logdir: /root/ray_results/training_workflow\n",
            "Number of trials: 1 ({'TERMINATED': 1})\n",
            "TERMINATED trials:\n",
            " - training_workflow_0:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=16866], 1 s, 10 iter, nan rew\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[training_workflow_0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    }
  ]
}
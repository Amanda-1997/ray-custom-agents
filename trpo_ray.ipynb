{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trpo-ray.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denklewer/ray-custom-agents/blob/master/trpo_ray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAGWDBl0Cxk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install -y build-essential curl unzip psmisc\n",
        "!pip install cython==0.29.0\n",
        "!git clone https://github.com/ray-project/ray.git\n",
        "!ray/ci/travis/install-bazel.sh\n",
        "!pip install lz4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiyjKhbxDm2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ray ray-distr\n",
        "!pip install -e ray-distr/python/. --verbose  # Add --user if you see a permission denied error."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSOf6g3sfOP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(\"\", 'ray-distr/python')) # To find local version of the library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnt6nQ4S5ju-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "d59e053d-29cb-4034-e7fe-7507817484e3"
      },
      "source": [
        "!rm -r ray-custom-agents\n",
        "!git clone https://github.com/denklewer/ray-custom-agents.git\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ray-custom-agents'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 28 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "757HuIw26WiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sys.path.append(os.path.join(\"\", 'ray-custom-agents')) # To find local version of the library\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doSKKtvv7cem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import trpo\n",
        "import gym\n",
        "import ray\n",
        "from trpo import TRPOAgent, DEFAULT_CONFIG\n",
        "from ray.tune.logger import pretty_print\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Md_idzJ7jz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "1ea7df14-1c0d-475b-a0ce-c9db0fdeb1f0"
      },
      "source": [
        "ray.init(ignore_reinit_error=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-17 21:15:24,677\tWARNING worker.py:1341 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
            "2019-05-17 21:15:24,679\tERROR worker.py:1347 -- Calling ray.init() again after it has already been called.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2a04ovk7moE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "7ea40497-da35-41d4-ad23-03c0b90c4840"
      },
      "source": [
        "config = DEFAULT_CONFIG.copy()\n",
        "config['num_workers'] = 2\n",
        "config['num_sgd_iter'] = 30\n",
        "config['num_gpus'] = 3\n",
        "config['sgd_minibatch_size'] = 128\n",
        "config['model']['fcnet_hiddens'] = [100, 100]\n",
        "config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
        "\n",
        "agent = TRPOAgent(config, 'CartPole-v0')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-17 21:15:28,167\tWARNING __init__.py:22 -- DeprecationWarning: TRPOAgent has been renamed to TRPOTrainer. This will raise an error in the future.\n",
            "FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
            "2019-05-17 21:15:28,189\tINFO policy_evaluator.py:312 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "2019-05-17 21:15:29,282\tINFO policy_evaluator.py:732 -- Built policy map: {'default_policy': <trpo.trpo_policy_graph.TRPOPolicyGraph object at 0x7fcddc8ee390>}\n",
            "2019-05-17 21:15:29,283\tINFO policy_evaluator.py:733 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fcddc1924a8>}\n",
            "2019-05-17 21:15:29,285\tINFO policy_evaluator.py:344 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fced85ce6d8>}\n",
            "2019-05-17 21:15:29,332\tINFO multi_gpu_optimizer.py:80 -- LocalMultiGPUOptimizer devices ['/gpu:0', '/gpu:1', '/gpu:2']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=12032)\u001b[0m 2019-05-17 21:15:29,652\tWARNING compression.py:20 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=12037)\u001b[0m 2019-05-17 21:15:29,683\tWARNING compression.py:20 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=12032)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=12032)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=12032)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=12037)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=12037)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=12037)\u001b[0m non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-17 21:15:39,491\tERROR worker.py:1616 -- Possible unhandled error from worker: \u001b[36mray_worker\u001b[39m (pid=12032, host=f94ada20656f)\n",
            "  File \"pyarrow/serialization.pxi\", line 458, in pyarrow.lib.deserialize\n",
            "  File \"pyarrow/serialization.pxi\", line 421, in pyarrow.lib.deserialize_from\n",
            "  File \"pyarrow/serialization.pxi\", line 272, in pyarrow.lib.SerializedPyObject.deserialize\n",
            "  File \"pyarrow/serialization.pxi\", line 171, in pyarrow.lib.SerializationContext._deserialize_callback\n",
            "ModuleNotFoundError: No module named 'trpo'\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNgdi5mK7pLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2998
        },
        "outputId": "d7136d5a-de7f-4730-dade-be7df5d3ee24"
      },
      "source": [
        "for i in range(2):\n",
        "    result = agent.train()\n",
        "    print(pretty_print(result))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,514\tINFO policy_evaluator.py:437 -- Generating sample batch of size 200\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,515\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.031, max=0.005, mean=-0.013)}}\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,515\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,515\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.031, max=0.005, mean=-0.013)\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,515\tINFO sampler.py:411 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.031, max=0.005, mean=-0.013)\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,516\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                                   'env_id': 0,\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                                   'info': None,\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.031, max=0.005, mean=-0.013),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                                   'prev_reward': 0.0,\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                                   'rnn_state': []},\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,516\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,546\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                       [],\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,567\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((15,), dtype=float32, min=0.499, max=0.501, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'actions': np.ndarray((15,), dtype=int64, min=0.0, max=1.0, mean=0.267),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'advantages': np.ndarray((15,), dtype=float32, min=0.996, max=13.994, mean=7.636),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'agent_index': np.ndarray((15,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'behaviour_logits': np.ndarray((15, 2), dtype=float32, min=-0.002, max=0.001, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'dones': np.ndarray((15,), dtype=bool, min=0.0, max=1.0, mean=0.067),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'eps_id': np.ndarray((15,), dtype=int64, min=1453988367.0, max=1453988367.0, mean=1453988367.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'infos': np.ndarray((15,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'new_obs': np.ndarray((15, 4), dtype=float32, min=-1.374, max=2.326, mean=0.089),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'obs': np.ndarray((15, 4), dtype=float32, min=-1.178, max=1.978, mean=0.071),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'prev_actions': np.ndarray((15,), dtype=int64, min=0.0, max=1.0, mean=0.267),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'prev_rewards': np.ndarray((15,), dtype=float32, min=0.0, max=1.0, mean=0.933),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'rewards': np.ndarray((15,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         't': np.ndarray((15,), dtype=int64, min=0.0, max=14.0, mean=7.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'unroll_id': np.ndarray((15,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'value_targets': np.ndarray((15,), dtype=float32, min=1.0, max=13.994, mean=7.639),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m                         'vf_preds': np.ndarray((15,), dtype=float32, min=-0.001, max=0.004, mean=0.002)},\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m               'type': 'SampleBatch'}}\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m 2019-05-17 13:19:13,856\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.499, max=0.501, mean=0.5),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.455),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=0.996, max=42.465, mean=13.826),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.002, max=0.002, mean=-0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.04),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=10457887.0, max=1553378998.0, mean=921252522.68),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.176, max=2.455, mean=0.067),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.08, max=2.358, mean=0.061),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.45),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.955),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=54.0, mean=14.61),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.999, max=42.465, mean=13.826),\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.007, max=0.005, mean=0.0)},\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m   'type': 'SampleBatch'}\n",
            "\u001b[2m\u001b[36m(pid=500)\u001b[0m \n",
            "custom_metrics: {}\n",
            "date: 2019-05-17_13-19-21\n",
            "done: false\n",
            "episode_len_mean: 20.921465968586386\n",
            "episode_reward_max: 70.0\n",
            "episode_reward_mean: 20.921465968586386\n",
            "episode_reward_min: 8.0\n",
            "episodes_this_iter: 191\n",
            "episodes_total: 191\n",
            "experiment_id: d887f1ccf59e43759840cfeee44e7c79\n",
            "hostname: 91e8c525c29f\n",
            "info:\n",
            "  grad_time_ms: 5093.369\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.19999995827674866\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6639507412910461\n",
            "      kl: 0.030143054202198982\n",
            "      policy_loss: -0.047729384154081345\n",
            "      total_loss: 140.85000610351562\n",
            "      vf_explained_var: 0.031812701374292374\n",
            "      vf_loss: 140.89170837402344\n",
            "  load_time_ms: 127.657\n",
            "  num_steps_sampled: 4000\n",
            "  num_steps_trained: 3906\n",
            "  sample_time_ms: 2951.863\n",
            "  update_time_ms: 1489.922\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 138\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.08486223897171379\n",
            "  mean_inference_ms: 1.0863349531105788\n",
            "  mean_processing_ms: 0.2516501848616522\n",
            "time_since_restore: 9.80753231048584\n",
            "time_this_iter_s: 9.80753231048584\n",
            "time_total_s: 9.80753231048584\n",
            "timestamp: 1558099161\n",
            "timesteps_since_restore: 4000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 4000\n",
            "training_iteration: 1\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2019-05-17_13-19-28\n",
            "done: false\n",
            "episode_len_mean: 40.88\n",
            "episode_reward_max: 168.0\n",
            "episode_reward_mean: 40.88\n",
            "episode_reward_min: 11.0\n",
            "episodes_this_iter: 94\n",
            "episodes_total: 285\n",
            "experiment_id: d887f1ccf59e43759840cfeee44e7c79\n",
            "hostname: 91e8c525c29f\n",
            "info:\n",
            "  grad_time_ms: 4522.658\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.30000004172325134\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6258705854415894\n",
            "      kl: 0.013685395941138268\n",
            "      policy_loss: 0.02991105243563652\n",
            "      total_loss: 281.8683776855469\n",
            "      vf_explained_var: 0.013888705521821976\n",
            "      vf_loss: 281.8343811035156\n",
            "  load_time_ms: 64.534\n",
            "  num_steps_sampled: 8000\n",
            "  num_steps_trained: 7812\n",
            "  sample_time_ms: 2926.755\n",
            "  update_time_ms: 749.791\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 2\n",
            "num_metric_batches_dropped: 0\n",
            "off_policy_estimator: {}\n",
            "pid: 138\n",
            "policy_reward_mean: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.08731857056738555\n",
            "  mean_inference_ms: 1.0862648915641406\n",
            "  mean_processing_ms: 0.23660922918351154\n",
            "time_since_restore: 16.683863401412964\n",
            "time_this_iter_s: 6.876331090927124\n",
            "time_total_s: 16.683863401412964\n",
            "timestamp: 1558099168\n",
            "timesteps_since_restore: 8000\n",
            "timesteps_this_iter: 4000\n",
            "timesteps_total: 8000\n",
            "training_iteration: 2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHrL98S57uIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
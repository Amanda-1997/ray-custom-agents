{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/denklewer/ray-custom-agents/blob/master/trpo_ray_ray_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54686
    },
    "colab_type": "code",
    "id": "iAGWDBl0Cxk6",
    "outputId": "757439df-b2bb-4867-8831-18f3cb9fa181"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y build-essential curl unzip psmisc\n",
    "!pip install cython==0.29.0\n",
    "!git clone https://github.com/ray-project/ray.git\n",
    "!ray/ci/travis/install-bazel.sh\n",
    "!pip install lz4\n",
    "!pip install setproctitle\n",
    "!mv ray ray-distr\n",
    "!pip install -e ray-distr/python/. --verbose  # Add --user if you see a permission denied error.\n",
    "!apt-get install swig\n",
    "!apt install -y python3-dev zlib1g-dev libjpeg-dev cmake swig python-pyglet python3-opengl libboost-all-dev libsdl2-dev \\\n",
    "    libosmesa6-dev patchelf ffmpeg xvfb\n",
    "!pip3 install box2d box2d-kengz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSOf6g3sfOP5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(\"\", '/content/ray-distr/python')) # To find local version of the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1u35Ns4e_-Ea"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def conv2d_size_out(size, kernel_size=3, stride=2):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "Z_iCFvpCJJLB",
    "outputId": "ceb499a3-b9c6-4dc0-dc6a-64528753473e"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import logging\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.evaluation import PolicyGraph, PolicyEvaluator,SampleBatch\n",
    "\n",
    "from ray.rllib.evaluation.metrics import collect_metrics\n",
    "from ray.rllib.models.pytorch.misc import  valid_padding\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "import scipy.signal\n",
    "from ray.rllib.utils.annotations import override\n",
    "logger = logging.getLogger(__name__)\n",
    "from ray.rllib.evaluation.postprocessing import compute_advantages,Postprocessing\n",
    "\n",
    "from torch.autograd import Variable\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--gpu\", action=\"store_true\")\n",
    "parser.add_argument(\"--num-iters\", type=int, default=20)\n",
    "parser.add_argument(\"--num-workers\", type=int, default=2)\n",
    "from ray.rllib.models.catalog import ModelCatalog, MODEL_DEFAULTS\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TRPOAgent(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, config,hidden_size=32):\n",
    "        '''\n",
    "        Here you should define your model\n",
    "        You should have LOG-PROBABILITIES as output because you will need it to compute loss\n",
    "        We recommend that you start simple:\n",
    "        use 1-2 hidden layers with 100-500 units and relu for the first try\n",
    "        '''\n",
    "        nn.Module.__init__(self)\n",
    "        config = defaultdict(**config)\n",
    "        self.config = config\n",
    "        self.n_actions = action_space.n\n",
    "        self.state_shape = observation_space.shape\n",
    "        self.action_space =action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.dist_class, self.logit_dim = ModelCatalog.get_action_dist(\n",
    "        self.action_space, None, torch=True)\n",
    "\n",
    "                \n",
    "        if isinstance(self.observation_space, gym.spaces.Discrete):\n",
    "            self.obs_rank = 1\n",
    "        else:\n",
    "            self.obs_rank = len(self.observation_space.shape)\n",
    "            \n",
    "        if self.obs_rank > 1:\n",
    "          w, h, c = (84, 84 , 4)\n",
    "          self.common = nn.Sequential(\n",
    "              nn.Conv2d(c, 32, kernel_size=3, stride=2),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(32, 32, kernel_size=3, stride=2),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(32, 32, kernel_size=3, stride=2),\n",
    "              nn.ReLU(),\n",
    "              Flatten())\n",
    "          convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "          convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "          linear_input_size = convw * convh * 32\n",
    "          # we want to flatten given images to verctor for dense layers.\n",
    "          self.fc = nn.Sequential(\n",
    "              nn.Linear(linear_input_size, 128),\n",
    "              nn.ReLU()\n",
    "          )\n",
    "          self.log_probs = nn.Sequential(\n",
    "              nn.Linear(128, self.n_actions),\n",
    "              nn.LogSoftmax()\n",
    "          )\n",
    "        else:\n",
    "          self.model = nn.Sequential(\n",
    "              nn.Linear(self.state_shape[0], hidden_size),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(hidden_size, self.n_actions),\n",
    "              nn.LogSoftmax()\n",
    "          )\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        takes agent's observation (Variable), returns log-probabilities (Variable)\n",
    "        :param state_t: a batch of states, shape = [batch_size, state_shape]\n",
    "        \"\"\"\n",
    "        \n",
    "                          \n",
    "        # Use your network to compute log_probs for given state\n",
    "        if self.obs_rank > 1:\n",
    "          states= states.permute(0,3,2,1)\n",
    "          state_t = self.common(states)\n",
    "          state_t = self.fc(state_t)\n",
    "          return self.log_probs(state_t)\n",
    "        else:\n",
    "          return self.model(states)\n",
    "\n",
    "    def get_log_probs(self, states):\n",
    "        '''\n",
    "        Log-probs for training\n",
    "        '''\n",
    "\n",
    "        return self.forward(states)\n",
    "\n",
    "    def get_probs(self, states):\n",
    "        '''\n",
    "        Probs for interaction\n",
    "        '''\n",
    "\n",
    "        return torch.exp(self.forward(states))\n",
    "\n",
    "    def act(self, obs, sample=True):\n",
    "        '''\n",
    "        Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n",
    "        :param: obs - single observation vector\n",
    "        :param sample: if True, samples from \\pi, otherwise takes most likely action\n",
    "        :returns: action (single integer) and probabilities for all actions\n",
    "        '''\n",
    "\n",
    "        probs = self.get_probs(Variable(torch.FloatTensor([obs]))).data.numpy()\n",
    "\n",
    "        if sample:\n",
    "            action = int(np.random.choice(self.n_actions, p=probs[0]))\n",
    "        else:\n",
    "            action = int(np.argmax(probs))\n",
    "\n",
    "        return action, probs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qxmLg0Gdmqg"
   },
   "outputs": [],
   "source": [
    "# UTIL FUNCTIONS\n",
    "def get_cummulative_returns(r, gamma=1):\n",
    "  \"\"\"\n",
    "  Computes cummulative discounted rewards given immediate rewards\n",
    "        G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
    "        Also known as R(s,a)\n",
    "  \"\"\"\n",
    "  r = np.array(r)\n",
    "  assert r.ndim >= 1\n",
    "  return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]\n",
    "\n",
    "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    This method solves system of equation Ax=b using iterative method called conjugate gradients\n",
    "    :f_Ax: function that returns Ax\n",
    "    :b: targets for Ax\n",
    "    :cg_iters: how many iterations this method should do\n",
    "    :residual_tol: epsilon for stability\n",
    "    \"\"\"\n",
    "    p = b.clone()\n",
    "    r = b.clone()\n",
    "    x = torch.zeros(b.size())\n",
    "    rdotr = torch.sum(r * r)\n",
    "    for i in range(cg_iters):\n",
    "        z = f_Ax(p)\n",
    "        v = rdotr / (torch.sum(p * z) + 1e-8)\n",
    "        x += v * p\n",
    "        r -= v * z\n",
    "        newrdotr = torch.sum(r * r)\n",
    "        mu = newrdotr / (rdotr + 1e-8)\n",
    "        p = r + mu * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioP5xoJbJJZu"
   },
   "outputs": [],
   "source": [
    "class CustomPolicy(PolicyGraph):\n",
    "    \"\"\"Example of a custom policy graph written from scratch.\n",
    "    You might find it more convenient to extend TF/TorchPolicyGraph instead\n",
    "    for a real policy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        PolicyGraph.__init__(self, observation_space, action_space, config)\n",
    "        # example parameter\n",
    "        self.w = 1.0\n",
    "        self.observation_shape = observation_space.shape\n",
    "        self.n_actions = action_space.n\n",
    "        self.agent = TRPOAgent(observation_space, action_space, config)\n",
    "        self.policy = []\n",
    "        \n",
    "\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        # return random actions\n",
    "        actions = []\n",
    "        action_probs = []\n",
    "        for obs in obs_batch:\n",
    "            action, policy = self.agent.act(obs)\n",
    "            actions.append(action)\n",
    "            action_probs.append(policy)\n",
    "\n",
    "        return actions, [], {\"action_probs\": action_probs}\n",
    "      \n",
    "    @override(PolicyGraph)\n",
    "    def postprocess_trajectory(self,\n",
    "                               sample_batch,\n",
    "                               other_agent_batches=None,\n",
    "                               episode=None):\n",
    "        traj = {}\n",
    "        for key in sample_batch:\n",
    "            traj[key] = np.stack(sample_batch[key])\n",
    "        traj[\"cummulative_returns\"] = get_cummulative_returns(traj[SampleBatch.REWARDS], gamma= 0.99)\n",
    "        return SampleBatch(traj)\n",
    "\n",
    "\n",
    "    def get_flat_params_from(self, model):\n",
    "        params = []\n",
    "        for param in model.parameters():\n",
    "            params.append(param.data.view(-1))\n",
    "\n",
    "        flat_params = torch.cat(params)\n",
    "        return flat_params\n",
    "\n",
    "    def set_flat_params_to(self, model, flat_params):\n",
    "        prev_ind = 0\n",
    "        for param in model.parameters():\n",
    "            flat_size = int(np.prod(list(param.size())))\n",
    "            param.data.copy_(\n",
    "                flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
    "            prev_ind += flat_size\n",
    "\n",
    "    def get_loss(self, agent, observations, actions, cummulative_returns, old_probs):\n",
    "        \"\"\"\n",
    "        Computes TRPO objective\n",
    "        :param: observations - batch of observations\n",
    "        :param: actions - batch of actions\n",
    "        :param: cummulative_returns - batch of cummulative returns\n",
    "        :param: old_probs - batch of probabilities computed by old network\n",
    "        :returns: scalar value of the objective function\n",
    "        \"\"\"\n",
    "        batch_size = observations.shape[0]\n",
    "        log_probs_all = agent.get_log_probs(observations)\n",
    "        probs_all = torch.exp(log_probs_all)\n",
    "\n",
    "        probs_for_actions = probs_all[torch.arange(\n",
    "            0, batch_size, out=torch.LongTensor()), actions]\n",
    "        old_probs_for_actions = old_probs[torch.arange(\n",
    "            0, batch_size, out=torch.LongTensor()), actions]\n",
    "\n",
    "        # Compute surrogate loss, aka importance-sampled policy gradient\n",
    "        Loss = -torch.mean(cummulative_returns * (probs_for_actions / old_probs_for_actions))\n",
    "\n",
    "        return Loss\n",
    "\n",
    "    def get_kl(self, agent, observations, actions, cummulative_returns, old_probs_all):\n",
    "        \"\"\"\n",
    "        Computes KL-divergence between network policy and old policy\n",
    "        :param: observations - batch of observations\n",
    "        :param: actions - batch of actions\n",
    "        :param: cummulative_returns - batch of cummulative returns (we don't need it actually)\n",
    "        :param: old_probs - batch of probabilities computed by old network\n",
    "        :returns: scalar value of the KL-divergence\n",
    "        \"\"\"\n",
    "        batch_size = observations.shape[0]\n",
    "        log_probs_all = agent.get_log_probs(observations)\n",
    "        probs_all = torch.exp(log_probs_all)\n",
    "\n",
    "        # Compute Kullback-Leibler divergence (see formula above)\n",
    "        # Note: you need to sum KL and entropy over all actions, not just the ones agent took\n",
    "        old_log_probs_all = torch.log(old_probs_all + 1e-10)\n",
    "\n",
    "        kl = torch.sum(old_probs_all * (old_log_probs_all - log_probs_all)) / batch_size\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def get_entropy(self, agent, observations):\n",
    "        \"\"\"\n",
    "        Computes entropy of the network policy\n",
    "        :param: observations - batch of observations\n",
    "        :returns: scalar value of the entropy\n",
    "        \"\"\"\n",
    "\n",
    "        observations = Variable(torch.FloatTensor(observations))\n",
    "\n",
    "        batch_size = observations.shape[0]\n",
    "        log_probs_all = agent.get_log_probs(observations)\n",
    "        probs_all = torch.exp(log_probs_all)\n",
    "\n",
    "        entropy = torch.sum(-probs_all * log_probs_all) / batch_size\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def linesearch(self, f, x, fullstep, max_kl):\n",
    "        \"\"\"\n",
    "        Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n",
    "        :param: f - function that returns loss, kl and arbitrary third component.\n",
    "        :param: x - old parameters of neural network.\n",
    "        :param: fullstep - direction in which we make search.\n",
    "        :param: max_kl - constraint of KL divergence.\n",
    "        :returns:\n",
    "        \"\"\"\n",
    "        max_backtracks = 10\n",
    "        loss, _, = f(x)\n",
    "        for stepfrac in .5 ** np.arange(max_backtracks):\n",
    "            xnew = x + stepfrac * fullstep\n",
    "            new_loss, kl = f(xnew)\n",
    "            actual_improve = new_loss - loss\n",
    "            if kl.data.numpy() <= max_kl and actual_improve.data.numpy() < 0:\n",
    "                x = xnew\n",
    "                loss = new_loss\n",
    "        return x\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        # implement your learning code here\n",
    "        max_kl = 0.01\n",
    "        observations = samples['obs']\n",
    "        actions = samples['actions']\n",
    "        returns = samples['cummulative_returns']\n",
    "        old_probs = samples['action_probs']\n",
    "        loss, kl = self.update_step(observations, actions, returns, old_probs, max_kl)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"kl\": kl\n",
    "        }\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.get_flat_params_from(self.agent)\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.set_flat_params_to(self.agent, weights)\n",
    "        \n",
    "    def update_step(self, observations, actions, cummulative_returns, old_probs, max_kl):\n",
    "      \"\"\"\n",
    "      This function does the TRPO update step\n",
    "      :param: observations - batch of observations\n",
    "      :param: actions - batch of actions\n",
    "      :param: cummulative_returns - batch of cummulative returns\n",
    "      :param: old_probs - batch of probabilities computed by old network\n",
    "      :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n",
    "      :returns: KL between new and old policies and the value of the loss function.\n",
    "      \"\"\"\n",
    "      agent = self.agent\n",
    "\n",
    "      # Here we prepare the information\n",
    "      observations = Variable(torch.FloatTensor(observations))\n",
    "      actions = torch.LongTensor(actions)\n",
    "      cummulative_returns = Variable(torch.FloatTensor(cummulative_returns))\n",
    "      old_probs = Variable(torch.FloatTensor(old_probs))\n",
    "\n",
    "      # Here we compute gradient of the loss function\n",
    "      loss = self.get_loss(agent, observations, actions,\n",
    "                      cummulative_returns, old_probs)\n",
    "\n",
    "      grads = torch.autograd.grad(loss, agent.parameters())\n",
    "      loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
    "\n",
    "      def Fvp(v):\n",
    "          # Here we compute Fx to do solve Fx = g using conjugate gradients\n",
    "          # We actually do here a couple of tricks to compute it efficiently\n",
    "\n",
    "          kl = self.get_kl(agent, observations, actions,\n",
    "                      cummulative_returns, old_probs)\n",
    "\n",
    "          grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n",
    "          flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "          kl_v = (flat_grad_kl * Variable(v)).sum()\n",
    "         \n",
    "          grads = torch.autograd.grad(kl_v, agent.parameters())\n",
    "          flat_grad_grad_kl = torch.cat(\n",
    "              [grad.contiguous().view(-1) for grad in grads]).data\n",
    "\n",
    "          return flat_grad_grad_kl + v * 0.1\n",
    "\n",
    "      # Here we solveolve Fx = g system using conjugate gradients\n",
    "      stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n",
    "\n",
    "      # Here we compute the initial vector to do linear search\n",
    "      shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
    "\n",
    "      lm = torch.sqrt(shs / max_kl)\n",
    "      fullstep = stepdir / lm[0]\n",
    "\n",
    "\n",
    "      # Here we get the start point\n",
    "      prev_params = self.get_weights()\n",
    "\n",
    "      def get_loss_kl(params):\n",
    "          # Helper for linear search\n",
    "          # Set new params and return loss + kl\n",
    "          self.set_weights(params)\n",
    "          return [self.get_loss(agent, observations, actions, cummulative_returns, old_probs),\n",
    "                  self.get_kl(agent, observations, actions, cummulative_returns, old_probs)]\n",
    "\n",
    "      # Here we find our new parameters\n",
    "      new_params = self.linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n",
    "\n",
    "      return get_loss_kl(new_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ll39xhuvJJXL"
   },
   "outputs": [],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "from collections import OrderedDict\n",
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "batch_steps_cnt = 1000\n",
    "\n",
    "def training_workflow(config, reporter):\n",
    "    env_name = \"CartPole-v1\"\n",
    "    # Setup policy and policy evaluation actors\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
    "    workers = [\n",
    "        PolicyEvaluator.as_remote().remote(\n",
    "            lambda c: gym.make(env_name),\n",
    "            CustomPolicy,\n",
    "            batch_steps=batch_steps_cnt,\n",
    "            model_config= MODEL_DEFAULTS\n",
    "        )\n",
    "        for _ in range(config[\"num_workers\"])\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    reward_history = []\n",
    "    std_history = []\n",
    "    sampling_time = 0\n",
    "    stats_step = config[\"num_iters\"] / 10\n",
    "    for it in trange(config[\"num_iters\"]):\n",
    "        # Broadcast weights to the policy evaluation workers\n",
    "        weights = ray.put({\"default_policy\": policy.get_weights()})\n",
    "        for w in workers:\n",
    "            w.set_weights.remote(weights)\n",
    "        start_sampling = time.time()\n",
    "\n",
    "        # Gather a batch of samples\n",
    "        samples = SampleBatch.concat_samples(ray.get([w.sample.remote() for w in workers]))\n",
    "        \n",
    "        sampling_time += (time.time() - start_sampling)\n",
    "        \n",
    "        loss_stats =   policy.learn_on_batch(samples)\n",
    "        # Report current progress\n",
    "        \n",
    "        paths = samples.split_by_episode()\n",
    "        episode_lengths = [len(path[SampleBatch.ACTIONS]) for path in paths]\n",
    "        episode_sum_rewards = np.array([path[SampleBatch.REWARDS].sum() for path in paths])\n",
    "        \n",
    "       \n",
    "        reward_history.append(episode_sum_rewards.mean())\n",
    "        std_history.append(episode_sum_rewards.std())\n",
    "        result =collect_metrics(remote_evaluators=workers)\n",
    "        if (it+2) % stats_step == 0:\n",
    "            stats = OrderedDict()\n",
    "            stats[\"Sampled steps\"] = len(samples[SampleBatch.ACTIONS])\n",
    "            stats[\"Episode Lengths\"] = episode_lengths\n",
    "            stats[\"Sum of rewards for episodes\"] = episode_sum_rewards\n",
    "            stats[\"Average sum of rewards per episode\"] = episode_sum_rewards.mean()\n",
    "            stats[\"Std of rewards per episode\"] = episode_sum_rewards.std()\n",
    "            stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
    "            stats[\"Sampling time\"] = \"%.2f mins\" % ((sampling_time)/60.)\n",
    "            stats[\"KL between old and new distribution\"] = loss_stats['kl'].data.numpy()\n",
    "            stats[\"Surrogate loss\"] = loss_stats['loss'].data.numpy()\n",
    "            clear_output(True)\n",
    "            for k, v in stats.items():\n",
    "                print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n",
    "            print(pretty_print(result))\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "#         prev_t = 0\n",
    "#         paths = []\n",
    "#         counter = 0\n",
    "#         obervations, actions, rewards, action_probs, cum_returns = [], [], [], [], []\n",
    "#         for t in samples['t']:\n",
    "#           if t == 0 and prev_t != 0:\n",
    "            \n",
    "#             path = {\"observations\": np.array(obervations),\n",
    "#                     \"policy\": np.array(action_probs),\n",
    "#                     \"actions\": np.array(actions),\n",
    "#                     \"rewards\": np.array(rewards),\n",
    "#                     \"cumulative_returns\": np.array(cum_returns),\n",
    "#                     }\n",
    "#             obervations, actions, rewards, action_probs = [], [], [], []\n",
    "#             paths.append(path)\n",
    "#           else:  \n",
    "#             obervations.append(samples['obs'][counter])\n",
    "#             actions.append(samples['actions'][counter])\n",
    "#             action_probs.append(samples['action_probs'][counter])\n",
    "#             rewards.append(samples['rewards'][counter])   \n",
    "#             cum_returns.append(samples['cummulative_returns'][counter])\n",
    "#             prev_t = t\n",
    "#           counter+=1\n",
    "\n",
    "        # Improve the policy using the  batch\n",
    "\n",
    "      \n",
    "    with open(\"/home/denklewer/Documents/master_test/\"+ env_name+\"_rew_hist_\"+ str(config[\"num_workers\"]) +\"_\"+ str(batch_steps_cnt) + \"_200.txt\", \"wb\") as fp:\n",
    "      pickle.dump(reward_history, fp)\n",
    "    with open(\"/home/denklewer/Documents/master_test/\"+ env_name+\"_rew_std_\"+ str(config[\"num_workers\"]) +\"_\"+ str(batch_steps_cnt) +\"_200.txt\", \"wb\") as fp:\n",
    "      pickle.dump(std_history, fp)\n",
    "    with open(\"/home/denklewer/Documents/master_test/\"+ env_name+\"_weights_\"+ str(config[\"num_workers\"]) +\"_\"+ str(batch_steps_cnt) + \"_200.txt\", \"wb\") as fp:\n",
    "      pickle.dump(policy.get_weights(), fp)\n",
    " \n",
    "    reporter(**collect_metrics(remote_evaluators=workers))\n",
    "\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "colab_type": "code",
    "id": "Ds31NxLhw-xQ",
    "outputId": "947d1859-a1d1-43fc-e7a3-b2ca64548172"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-22 16:06:09,625\tINFO node.py:497 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-22_16-06-09_625207_17494/logs.\n",
      "2019-05-22 16:06:09,736\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:51941 to respond...\n",
      "2019-05-22 16:06:09,855\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:44032 to respond...\n",
      "2019-05-22 16:06:09,861\tINFO services.py:806 -- Starting Redis shard with 1.65 GB max memory.\n",
      "2019-05-22 16:06:09,888\tINFO node.py:511 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-22_16-06-09_625207_17494/logs.\n",
      "2019-05-22 16:06:09,890\tINFO services.py:1441 -- Starting the Plasma object store with 2.47 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.100',\n",
       " 'object_store_address': '/tmp/ray/session_2019-05-22_16-06-09_625207_17494/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-05-22_16-06-09_625207_17494/sockets/raylet',\n",
       " 'redis_address': '192.168.0.100:51941',\n",
       " 'session_dir': '/tmp/ray/session_2019-05-22_16-06-09_625207_17494',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5623
    },
    "colab_type": "code",
    "id": "0XslGt--JJR1",
    "outputId": "32266a7b-674d-482b-9f4a-c981ca62336d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-22 18:45:13,497\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
      "2019-05-22 18:45:13,497\tINFO tune.py:223 -- Starting a new experiment.\n",
      "2019-05-22 18:45:13,514\tWARNING logger.py:130 -- Couldn't import TensorFlow - disabling TensorBoard logging.\n",
      "2019-05-22 18:45:13,515\tWARNING logger.py:224 -- Could not instantiate <class 'ray.tune.logger.TFLogger'> - skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.2 GB\n",
      "Result logdir: /home/denklewer/ray_results/training_workflow\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - training_workflow_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m 2019-05-22 18:45:13,803\tWARNING worker.py:204 -- Calling ray.get or ray.wait in a separate thread may lead to deadlock if the main thread blocks on this thread and there are not enough resources to execute more tasks\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,841\tINFO policy_evaluator.py:732 -- Built policy map: {'default_policy': <__main__.CustomPolicy object at 0x7f7efbec2898>}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,841\tINFO policy_evaluator.py:733 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f7efbec2860>}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,841\tINFO policy_evaluator.py:344 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f7efbec2828>}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,935\tINFO policy_evaluator.py:438 -- Generating sample batch of size 1000\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,936\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.025, max=0.026, mean=-0.004)}}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,936\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,936\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.025, max=0.026, mean=-0.004)\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,936\tINFO sampler.py:411 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.025, max=0.026, mean=-0.004)\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,938\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.025, max=0.026, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m /home/denklewer/anaconda3/envs/py3/lib/python3.5/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m   input = module(input)\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,940\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m { 'default_policy': ( [1],\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                       { 'action_probs': [ np.ndarray((2,), dtype=float32, min=0.453, max=0.547, mean=0.5)]})}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   0%|          | 0/400 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:13,972\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m { 'agent0': { 'data': { 'action_probs': np.ndarray((20, 2), dtype=float32, min=0.379, max=0.621, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'actions': np.ndarray((20,), dtype=int64, min=0.0, max=1.0, mean=0.7),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'cummulative_returns': np.ndarray((20,), dtype=float64, min=1.0, max=18.209, mean=9.864),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=226444666.0, max=226444666.0, mean=226444666.0),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'new_obs': np.ndarray((20, 4), dtype=float32, min=-2.675, max=1.597, mean=-0.084),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'obs': np.ndarray((20, 4), dtype=float32, min=-2.326, max=1.401, mean=-0.07),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'prev_actions': np.ndarray((20,), dtype=int64, min=0.0, max=1.0, mean=0.65),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=0.0, max=1.0, mean=0.95),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m 2019-05-22 18:45:15,990\tINFO policy_evaluator.py:475 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m { 'data': { 'action_probs': np.ndarray((1000, 2), dtype=float32, min=0.379, max=0.621, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.455),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'cummulative_returns': np.ndarray((1000,), dtype=float32, min=1.0, max=45.831, mean=13.174),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.043),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'eps_id': np.ndarray((1000,), dtype=int64, min=66200731.0, max=1983906925.0, mean=961325061.091),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'new_obs': np.ndarray((1000, 4), dtype=float32, min=-2.675, max=2.488, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'obs': np.ndarray((1000, 4), dtype=float32, min=-2.326, max=2.395, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'prev_actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.428),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'prev_rewards': np.ndarray((1000,), dtype=float32, min=0.0, max=1.0, mean=0.956),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'rewards': np.ndarray((1000,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             't': np.ndarray((1000,), dtype=int64, min=0.0, max=60.0, mean=13.709),\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m             'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=12873)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,132\tINFO policy_evaluator.py:732 -- Built policy map: {'default_policy': <__main__.CustomPolicy object at 0x7f548e36b6d8>}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,132\tINFO policy_evaluator.py:733 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f548e36b6a0>}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,132\tINFO policy_evaluator.py:344 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f548e36b668>}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,150\tINFO policy_evaluator.py:438 -- Generating sample batch of size 1000\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,151\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.007, max=0.013, mean=0.005)}}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,151\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,151\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.007, max=0.013, mean=0.005)\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,152\tINFO sampler.py:411 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.007, max=0.013, mean=0.005)\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,152\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.007, max=0.013, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m /home/denklewer/anaconda3/envs/py3/lib/python3.5/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m   input = module(input)\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,155\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m { 'default_policy': ( [1],\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                       { 'action_probs': [ np.ndarray((2,), dtype=float32, min=0.453, max=0.547, mean=0.5)]})}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:17,201\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m { 'agent0': { 'data': { 'action_probs': np.ndarray((24, 2), dtype=float32, min=0.4, max=0.6, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'actions': np.ndarray((24,), dtype=int64, min=0.0, max=1.0, mean=0.542),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'agent_index': np.ndarray((24,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'cummulative_returns': np.ndarray((24,), dtype=float64, min=1.0, max=21.432, mean=11.592),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'dones': np.ndarray((24,), dtype=bool, min=0.0, max=1.0, mean=0.042),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'eps_id': np.ndarray((24,), dtype=int64, min=1473551348.0, max=1473551348.0, mean=1473551348.0),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'infos': np.ndarray((24,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'new_obs': np.ndarray((24, 4), dtype=float32, min=-1.224, max=0.596, mean=-0.082),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'obs': np.ndarray((24, 4), dtype=float32, min=-1.108, max=0.596, mean=-0.072),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'prev_actions': np.ndarray((24,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'prev_rewards': np.ndarray((24,), dtype=float32, min=0.0, max=1.0, mean=0.958),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'rewards': np.ndarray((24,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         't': np.ndarray((24,), dtype=int64, min=0.0, max=23.0, mean=11.5),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m                         'unroll_id': np.ndarray((24,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,083\tINFO policy_evaluator.py:732 -- Built policy map: {'default_policy': <__main__.CustomPolicy object at 0x7f2a1eeb78d0>}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,083\tINFO policy_evaluator.py:733 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f2a1eeb7898>}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,083\tINFO policy_evaluator.py:344 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f2a1eeb7860>}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m 2019-05-22 18:45:18,170\tINFO policy_evaluator.py:475 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m { 'data': { 'action_probs': np.ndarray((1000, 2), dtype=float32, min=0.379, max=0.621, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.46),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'cummulative_returns': np.ndarray((1000,), dtype=float32, min=1.0, max=41.296, mean=14.004),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.04),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'eps_id': np.ndarray((1000,), dtype=int64, min=32681002.0, max=1940027875.0, mean=1049795940.555),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'new_obs': np.ndarray((1000, 4), dtype=float32, min=-2.926, max=2.776, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'obs': np.ndarray((1000, 4), dtype=float32, min=-2.582, max=2.648, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'prev_actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.439),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'prev_rewards': np.ndarray((1000,), dtype=float32, min=0.0, max=1.0, mean=0.959),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'rewards': np.ndarray((1000,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             't': np.ndarray((1000,), dtype=int64, min=0.0, max=52.0, mean=14.659),\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m             'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=7045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,190\tINFO policy_evaluator.py:438 -- Generating sample batch of size 1000\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,191\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.027, max=0.04, mean=-0.004)}}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,191\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,192\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.027, max=0.04, mean=-0.004)\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,192\tINFO sampler.py:411 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.027, max=0.04, mean=-0.004)\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,192\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.027, max=0.04, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m /home/denklewer/anaconda3/envs/py3/lib/python3.5/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m   input = module(input)\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,194\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m { 'default_policy': ( [1],\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                       { 'action_probs': [ np.ndarray((2,), dtype=float32, min=0.453, max=0.547, mean=0.5)]})}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,215\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m { 'agent0': { 'data': { 'action_probs': np.ndarray((42, 2), dtype=float32, min=0.429, max=0.571, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'actions': np.ndarray((42,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'agent_index': np.ndarray((42,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'cummulative_returns': np.ndarray((42,), dtype=float64, min=1.0, max=34.434, mean=18.834),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'dones': np.ndarray((42,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'eps_id': np.ndarray((42,), dtype=int64, min=1615283927.0, max=1615283927.0, mean=1615283927.0),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'infos': np.ndarray((42,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'new_obs': np.ndarray((42, 4), dtype=float32, min=-0.565, max=1.309, mean=0.052),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'obs': np.ndarray((42, 4), dtype=float32, min=-0.565, max=1.309, mean=0.047),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'prev_actions': np.ndarray((42,), dtype=int64, min=0.0, max=1.0, mean=0.476),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'prev_rewards': np.ndarray((42,), dtype=float32, min=0.0, max=1.0, mean=0.976),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'rewards': np.ndarray((42,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         't': np.ndarray((42,), dtype=int64, min=0.0, max=41.0, mean=20.5),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m                         'unroll_id': np.ndarray((42,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m /home/denklewer/anaconda3/envs/py3/lib/python3.5/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   input = module(input)\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m 2019-05-22 18:45:18,601\tINFO policy_evaluator.py:475 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m { 'data': { 'action_probs': np.ndarray((1000, 2), dtype=float32, min=0.388, max=0.612, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.438),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'cummulative_returns': np.ndarray((1000,), dtype=float32, min=1.0, max=55.695, mean=15.724),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.037),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'eps_id': np.ndarray((1000,), dtype=int64, min=5608528.0, max=1932436345.0, mean=953636269.465),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'new_obs': np.ndarray((1000, 4), dtype=float32, min=-2.129, max=2.574, mean=0.022),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'obs': np.ndarray((1000, 4), dtype=float32, min=-1.896, max=2.574, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'prev_actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.416),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'prev_rewards': np.ndarray((1000,), dtype=float32, min=0.0, max=1.0, mean=0.962),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'rewards': np.ndarray((1000,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             't': np.ndarray((1000,), dtype=int64, min=0.0, max=80.0, mean=17.061),\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m             'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=7047)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   0%|          | 1/400 [00:04<31:58,  4.81s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   0%|          | 2/400 [00:06<25:27,  3.84s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   1%|          | 3/400 [00:07<20:49,  3.15s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   1%|          | 4/400 [00:09<17:06,  2.59s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   1%|▏         | 5/400 [00:10<14:24,  2.19s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   2%|▏         | 6/400 [00:11<12:26,  1.90s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   2%|▏         | 7/400 [00:12<11:16,  1.72s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   2%|▏         | 8/400 [00:14<10:21,  1.59s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   2%|▏         | 9/400 [00:15<09:57,  1.53s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   2%|▎         | 10/400 [00:16<09:20,  1.44s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   3%|▎         | 11/400 [00:18<08:55,  1.38s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   3%|▎         | 12/400 [00:19<08:33,  1.32s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   3%|▎         | 13/400 [00:20<08:21,  1.30s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   4%|▎         | 14/400 [00:21<08:13,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   4%|▍         | 15/400 [00:23<08:07,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   4%|▍         | 16/400 [00:24<08:04,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   4%|▍         | 17/400 [00:25<08:00,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   4%|▍         | 18/400 [00:26<07:54,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   5%|▍         | 19/400 [00:27<07:50,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   5%|▌         | 20/400 [00:29<07:46,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   5%|▌         | 21/400 [00:30<07:46,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   6%|▌         | 22/400 [00:31<07:48,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   6%|▌         | 23/400 [00:32<07:44,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   6%|▌         | 24/400 [00:34<07:42,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   6%|▋         | 25/400 [00:35<07:39,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   6%|▋         | 26/400 [00:36<07:39,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   7%|▋         | 27/400 [00:37<07:36,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   7%|▋         | 28/400 [00:38<07:36,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   7%|▋         | 29/400 [00:40<07:35,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   8%|▊         | 30/400 [00:41<07:34,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   8%|▊         | 31/400 [00:42<07:32,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   8%|▊         | 32/400 [00:43<07:40,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   8%|▊         | 33/400 [00:45<07:50,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   8%|▊         | 34/400 [00:46<07:48,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   9%|▉         | 35/400 [00:47<07:58,  1.31s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   9%|▉         | 36/400 [00:49<07:52,  1.30s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   9%|▉         | 37/400 [00:50<07:48,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  10%|▉         | 38/400 [00:51<07:43,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [18, 313, 309, 308, 52, 116, 317, 303, 264, 313, 321, 309, 57]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [ 18. 313. 309. 308.  52. 116. 317. 303. 264. 313. 321. 309.  57.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       230.76923\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               115.76644\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             0.88 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            0.82 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.00997448\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -66.39061\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 314.6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 339.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 314.6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 297.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 10\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04410796958829637\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.2629035204050648\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09754614806664894\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  10%|▉         | 39/400 [00:52<07:32,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  10%|█         | 40/400 [00:54<07:24,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  10%|█         | 41/400 [00:55<07:16,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  10%|█         | 42/400 [00:56<07:11,  1.20s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  11%|█         | 43/400 [00:57<07:08,  1.20s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  11%|█         | 44/400 [00:58<07:10,  1.21s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  11%|█▏        | 45/400 [01:00<07:09,  1.21s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  12%|█▏        | 46/400 [01:01<07:17,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  12%|█▏        | 47/400 [01:02<07:18,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  12%|█▏        | 48/400 [01:03<07:19,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  12%|█▏        | 49/400 [01:05<07:30,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  12%|█▎        | 50/400 [01:06<07:30,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  13%|█▎        | 51/400 [01:07<07:19,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  13%|█▎        | 52/400 [01:09<07:14,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  13%|█▎        | 53/400 [01:10<07:11,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  14%|█▎        | 54/400 [01:11<07:11,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  14%|█▍        | 55/400 [01:12<07:18,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  14%|█▍        | 56/400 [01:14<08:02,  1.40s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  14%|█▍        | 57/400 [01:16<08:04,  1.41s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  14%|█▍        | 58/400 [01:17<07:45,  1.36s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  15%|█▍        | 59/400 [01:18<07:30,  1.32s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  15%|█▌        | 60/400 [01:19<07:18,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  15%|█▌        | 61/400 [01:20<07:09,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  16%|█▌        | 62/400 [01:22<07:04,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  16%|█▌        | 63/400 [01:23<06:59,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  16%|█▌        | 64/400 [01:24<06:56,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  16%|█▋        | 65/400 [01:25<06:52,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  16%|█▋        | 66/400 [01:27<06:52,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  17%|█▋        | 67/400 [01:28<06:49,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  17%|█▋        | 68/400 [01:29<06:49,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  17%|█▋        | 69/400 [01:30<06:47,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  18%|█▊        | 70/400 [01:31<06:44,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  18%|█▊        | 71/400 [01:33<06:43,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  18%|█▊        | 72/400 [01:34<06:43,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  18%|█▊        | 73/400 [01:35<06:41,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  18%|█▊        | 74/400 [01:36<06:40,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  19%|█▉        | 75/400 [01:38<06:38,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  19%|█▉        | 76/400 [01:39<06:36,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  19%|█▉        | 77/400 [01:40<06:33,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  20%|█▉        | 78/400 [01:41<06:31,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [451, 500, 49, 451, 411, 138, 209, 500, 291]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [451. 500.  49. 451. 411. 138. 209. 500. 291.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       333.33334\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               158.3\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             1.72 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            1.58 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009969891\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -74.33593\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 485.1666666666667\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 500.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 485.1666666666667\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 411.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04233972557784027\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.2531630907089982\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09334398670119033\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  20%|█▉        | 79/400 [01:42<06:30,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  20%|██        | 80/400 [01:44<06:32,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  20%|██        | 81/400 [01:45<06:34,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  20%|██        | 82/400 [01:46<06:31,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  21%|██        | 83/400 [01:47<06:29,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  21%|██        | 84/400 [01:49<06:27,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  21%|██▏       | 85/400 [01:50<06:24,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  22%|██▏       | 86/400 [01:51<06:24,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  22%|██▏       | 87/400 [01:52<06:22,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  22%|██▏       | 88/400 [01:54<06:25,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  22%|██▏       | 89/400 [01:55<06:23,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  22%|██▎       | 90/400 [01:56<06:21,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  23%|██▎       | 91/400 [01:57<06:20,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  23%|██▎       | 92/400 [01:58<06:19,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  23%|██▎       | 93/400 [02:00<06:16,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  24%|██▎       | 94/400 [02:01<06:13,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  24%|██▍       | 95/400 [02:02<06:12,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  24%|██▍       | 96/400 [02:03<06:10,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  24%|██▍       | 97/400 [02:05<06:36,  1.31s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  24%|██▍       | 98/400 [02:06<06:52,  1.37s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  25%|██▍       | 99/400 [02:08<06:39,  1.33s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  25%|██▌       | 100/400 [02:09<06:27,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  25%|██▌       | 101/400 [02:10<06:21,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  26%|██▌       | 102/400 [02:11<06:17,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  26%|██▌       | 103/400 [02:12<06:11,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  26%|██▌       | 104/400 [02:14<06:09,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  26%|██▋       | 105/400 [02:15<06:07,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  26%|██▋       | 106/400 [02:16<06:04,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  27%|██▋       | 107/400 [02:17<06:01,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  27%|██▋       | 108/400 [02:19<05:59,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  27%|██▋       | 109/400 [02:20<05:58,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  28%|██▊       | 110/400 [02:21<05:56,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  28%|██▊       | 111/400 [02:22<05:55,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  28%|██▊       | 112/400 [02:24<05:52,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  28%|██▊       | 113/400 [02:25<05:53,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  28%|██▊       | 114/400 [02:26<05:51,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  29%|██▉       | 115/400 [02:27<05:51,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  29%|██▉       | 116/400 [02:28<05:49,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  29%|██▉       | 117/400 [02:30<05:48,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  30%|██▉       | 118/400 [02:31<05:47,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [17, 500, 483, 358, 500, 142, 486, 500, 14]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [ 17. 500. 483. 358. 500. 142. 486. 500.  14.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       333.33334\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               202.29187\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             2.54 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            2.34 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009997375\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -77.27352\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 478.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 500.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 478.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 368.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04179179844567612\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.2500802417177616\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09209286812877422\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  30%|██▉       | 119/400 [02:32<05:45,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  30%|███       | 120/400 [02:33<05:49,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  30%|███       | 121/400 [02:35<06:02,  1.30s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  30%|███       | 122/400 [02:36<05:56,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  31%|███       | 123/400 [02:37<05:52,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  31%|███       | 124/400 [02:39<05:50,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  31%|███▏      | 125/400 [02:40<05:45,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  32%|███▏      | 126/400 [02:41<05:41,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  32%|███▏      | 127/400 [02:42<05:37,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  32%|███▏      | 128/400 [02:44<05:37,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  32%|███▏      | 129/400 [02:45<05:34,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  32%|███▎      | 130/400 [02:46<05:33,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  33%|███▎      | 131/400 [02:47<05:31,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  33%|███▎      | 132/400 [02:48<05:30,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  33%|███▎      | 133/400 [02:50<05:27,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  34%|███▎      | 134/400 [02:51<05:26,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  34%|███▍      | 135/400 [02:52<05:25,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  34%|███▍      | 136/400 [02:53<05:26,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  34%|███▍      | 137/400 [02:55<05:22,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  34%|███▍      | 138/400 [02:56<05:20,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  35%|███▍      | 139/400 [02:57<05:20,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  35%|███▌      | 140/400 [02:58<05:18,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  35%|███▌      | 141/400 [02:59<05:16,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  36%|███▌      | 142/400 [03:01<05:15,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  36%|███▌      | 143/400 [03:02<05:15,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  36%|███▌      | 144/400 [03:03<05:14,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  36%|███▋      | 145/400 [03:04<05:13,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  36%|███▋      | 146/400 [03:06<05:11,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  37%|███▋      | 147/400 [03:07<05:09,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  37%|███▋      | 148/400 [03:08<05:08,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  37%|███▋      | 149/400 [03:09<05:06,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  38%|███▊      | 150/400 [03:10<05:04,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  38%|███▊      | 151/400 [03:12<05:03,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  38%|███▊      | 152/400 [03:13<05:05,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  38%|███▊      | 153/400 [03:14<05:03,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  38%|███▊      | 154/400 [03:15<05:02,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  39%|███▉      | 155/400 [03:17<05:00,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  39%|███▉      | 156/400 [03:18<04:58,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  39%|███▉      | 157/400 [03:19<04:55,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  40%|███▉      | 158/400 [03:20<04:56,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [28, 305, 214, 453, 413, 297, 290, 92, 293, 500, 115]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [ 28. 305. 214. 453. 413. 297. 290.  92. 293. 500. 115.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       272.72726\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               143.66887\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             3.37 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            3.10 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009971377\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -70.039825\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 360.625\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 500.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 360.625\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 214.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 8\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04160030928896111\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.24902855824902675\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.091448231408157\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  40%|███▉      | 159/400 [03:21<04:54,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  40%|████      | 160/400 [03:23<04:54,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  40%|████      | 161/400 [03:24<04:55,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  40%|████      | 162/400 [03:25<04:53,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  41%|████      | 163/400 [03:26<04:51,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  41%|████      | 164/400 [03:28<04:50,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  41%|████▏     | 165/400 [03:29<04:50,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  42%|████▏     | 166/400 [03:30<04:48,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  42%|████▏     | 167/400 [03:31<04:47,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  42%|████▏     | 168/400 [03:33<04:46,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  42%|████▏     | 169/400 [03:34<04:43,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  42%|████▎     | 170/400 [03:35<04:41,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  43%|████▎     | 171/400 [03:36<04:41,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  43%|████▎     | 172/400 [03:38<04:39,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  43%|████▎     | 173/400 [03:39<04:39,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  44%|████▎     | 174/400 [03:40<04:39,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  44%|████▍     | 175/400 [03:41<04:37,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  44%|████▍     | 176/400 [03:42<04:35,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  44%|████▍     | 177/400 [03:44<04:34,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  44%|████▍     | 178/400 [03:45<04:33,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  45%|████▍     | 179/400 [03:46<04:30,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  45%|████▌     | 180/400 [03:47<04:27,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  45%|████▌     | 181/400 [03:49<04:26,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  46%|████▌     | 182/400 [03:50<04:26,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  46%|████▌     | 183/400 [03:51<04:24,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  46%|████▌     | 184/400 [03:52<04:23,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  46%|████▋     | 185/400 [03:53<04:22,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  46%|████▋     | 186/400 [03:55<04:21,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  47%|████▋     | 187/400 [03:56<04:19,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  47%|████▋     | 188/400 [03:57<04:18,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  47%|████▋     | 189/400 [03:58<04:17,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  48%|████▊     | 190/400 [04:00<04:16,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  48%|████▊     | 191/400 [04:01<04:17,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  48%|████▊     | 192/400 [04:02<04:15,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  48%|████▊     | 193/400 [04:03<04:14,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  48%|████▊     | 194/400 [04:04<04:13,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  49%|████▉     | 195/400 [04:06<04:11,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  49%|████▉     | 196/400 [04:07<04:10,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  49%|████▉     | 197/400 [04:08<04:09,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  50%|████▉     | 198/400 [04:09<04:07,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [309, 333, 339, 19, 94, 354, 357, 195, 209, 365, 385, 41]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [309. 333. 339.  19.  94. 354. 357. 195. 209. 365. 385.  41.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       250.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               128.48152\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             4.18 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            3.85 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009999927\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -68.82578\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 357.8888888888889\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 385.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 357.8888888888889\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 333.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 9\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04123295218876303\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.246856094337783\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09048795316608636\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  50%|████▉     | 199/400 [04:11<04:06,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  50%|█████     | 200/400 [04:12<04:03,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  50%|█████     | 201/400 [04:13<04:02,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  50%|█████     | 202/400 [04:14<04:03,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  51%|█████     | 203/400 [04:15<04:01,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  51%|█████     | 204/400 [04:17<03:58,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  51%|█████▏    | 205/400 [04:18<03:57,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  52%|█████▏    | 206/400 [04:19<03:57,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  52%|█████▏    | 207/400 [04:20<03:56,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  52%|█████▏    | 208/400 [04:22<03:55,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  52%|█████▏    | 209/400 [04:23<03:54,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  52%|█████▎    | 210/400 [04:24<03:52,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  53%|█████▎    | 211/400 [04:25<03:51,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  53%|█████▎    | 212/400 [04:26<03:49,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  53%|█████▎    | 213/400 [04:28<03:47,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  54%|█████▎    | 214/400 [04:29<03:46,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  54%|█████▍    | 215/400 [04:30<03:45,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  54%|█████▍    | 216/400 [04:31<03:43,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  54%|█████▍    | 217/400 [04:33<03:42,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  55%|█████▍    | 218/400 [04:34<03:42,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  55%|█████▍    | 219/400 [04:35<03:42,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  55%|█████▌    | 220/400 [04:36<03:40,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  55%|█████▌    | 221/400 [04:37<03:39,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  56%|█████▌    | 222/400 [04:39<03:37,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  56%|█████▌    | 223/400 [04:40<03:35,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  56%|█████▌    | 224/400 [04:41<03:34,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  56%|█████▋    | 225/400 [04:42<03:33,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  56%|█████▋    | 226/400 [04:44<03:34,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  57%|█████▋    | 227/400 [04:45<03:33,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  57%|█████▋    | 228/400 [04:46<03:31,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  57%|█████▋    | 229/400 [04:47<03:29,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  57%|█████▊    | 230/400 [04:48<03:27,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  58%|█████▊    | 231/400 [04:50<03:26,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  58%|█████▊    | 232/400 [04:51<03:24,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  58%|█████▊    | 233/400 [04:52<03:24,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  58%|█████▊    | 234/400 [04:53<03:23,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  59%|█████▉    | 235/400 [04:55<03:21,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  59%|█████▉    | 236/400 [04:56<03:20,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  59%|█████▉    | 237/400 [04:57<03:19,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  60%|█████▉    | 238/400 [04:58<03:17,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [321, 428, 251, 401, 415, 184, 353, 416, 231]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [321. 428. 251. 401. 415. 184. 353. 416. 231.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       333.33334\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               86.36743\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             5.00 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            4.60 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009982771\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -72.098816\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 405.8333333333333\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 428.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 405.8333333333333\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 373.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04105640967048863\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.2458414800656857\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.08998502239402266\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  60%|█████▉    | 239/400 [04:59<03:16,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  60%|██████    | 240/400 [05:01<03:14,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  60%|██████    | 241/400 [05:02<03:13,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  60%|██████    | 242/400 [05:03<03:16,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  61%|██████    | 243/400 [05:04<03:14,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  61%|██████    | 244/400 [05:06<03:12,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  61%|██████▏   | 245/400 [05:07<03:09,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  62%|██████▏   | 246/400 [05:08<03:08,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  62%|██████▏   | 247/400 [05:09<03:06,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  62%|██████▏   | 248/400 [05:11<03:05,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  62%|██████▏   | 249/400 [05:12<03:04,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  62%|██████▎   | 250/400 [05:13<03:04,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  63%|██████▎   | 251/400 [05:14<03:02,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  63%|██████▎   | 252/400 [05:15<03:01,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  63%|██████▎   | 253/400 [05:17<03:00,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  64%|██████▎   | 254/400 [05:18<02:59,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  64%|██████▍   | 255/400 [05:19<02:57,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  64%|██████▍   | 256/400 [05:20<02:56,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  64%|██████▍   | 257/400 [05:22<02:55,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  64%|██████▍   | 258/400 [05:23<02:56,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  65%|██████▍   | 259/400 [05:24<02:55,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  65%|██████▌   | 260/400 [05:25<02:53,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  65%|██████▌   | 261/400 [05:27<02:51,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  66%|██████▌   | 262/400 [05:28<02:50,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  66%|██████▌   | 263/400 [05:29<02:49,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  66%|██████▌   | 264/400 [05:30<02:48,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  66%|██████▋   | 265/400 [05:31<02:46,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  66%|██████▋   | 266/400 [05:33<02:44,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  67%|██████▋   | 267/400 [05:34<02:44,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  67%|██████▋   | 268/400 [05:35<02:42,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  67%|██████▋   | 269/400 [05:37<02:54,  1.33s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  68%|██████▊   | 270/400 [05:38<02:59,  1.38s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  68%|██████▊   | 271/400 [05:40<03:08,  1.46s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  68%|██████▊   | 272/400 [05:41<03:08,  1.47s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  68%|██████▊   | 273/400 [05:43<02:57,  1.40s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  68%|██████▊   | 274/400 [05:44<02:49,  1.35s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  69%|██████▉   | 275/400 [05:45<02:43,  1.31s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  69%|██████▉   | 276/400 [05:46<02:38,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  69%|██████▉   | 277/400 [05:47<02:35,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  70%|██████▉   | 278/400 [05:49<02:31,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [309, 491, 200, 443, 478, 79, 253, 472, 275]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [309. 491. 200. 443. 478.  79. 253. 472. 275.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       333.33334\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               137.41826\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             5.84 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            5.38 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009968961\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -73.31157\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 468.8333333333333\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 491.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 468.8333333333333\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 447.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.0411272944796962\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.2462513140428267\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09007395741114065\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  70%|██████▉   | 279/400 [05:50<02:29,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  70%|███████   | 280/400 [05:51<02:28,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  70%|███████   | 281/400 [05:52<02:28,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  70%|███████   | 282/400 [05:54<02:27,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  71%|███████   | 283/400 [05:55<02:25,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  71%|███████   | 284/400 [05:56<02:22,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  71%|███████▏  | 285/400 [05:57<02:22,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  72%|███████▏  | 286/400 [05:59<02:20,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  72%|███████▏  | 287/400 [06:00<02:19,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  72%|███████▏  | 288/400 [06:01<02:19,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  72%|███████▏  | 289/400 [06:02<02:17,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  72%|███████▎  | 290/400 [06:04<02:15,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  73%|███████▎  | 291/400 [06:05<02:14,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  73%|███████▎  | 292/400 [06:06<02:12,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  73%|███████▎  | 293/400 [06:07<02:11,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  74%|███████▎  | 294/400 [06:09<02:13,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  74%|███████▍  | 295/400 [06:10<02:27,  1.40s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  74%|███████▍  | 296/400 [06:12<02:21,  1.36s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  74%|███████▍  | 297/400 [06:13<02:17,  1.34s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  74%|███████▍  | 298/400 [06:14<02:15,  1.33s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  75%|███████▍  | 299/400 [06:15<02:11,  1.30s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  75%|███████▌  | 300/400 [06:17<02:08,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  75%|███████▌  | 301/400 [06:18<02:05,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  76%|███████▌  | 302/400 [06:19<02:03,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  76%|███████▌  | 303/400 [06:20<02:01,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  76%|███████▌  | 304/400 [06:22<02:02,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  76%|███████▋  | 305/400 [06:23<02:02,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  76%|███████▋  | 306/400 [06:24<01:59,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  77%|███████▋  | 307/400 [06:25<01:56,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  77%|███████▋  | 308/400 [06:27<01:56,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  77%|███████▋  | 309/400 [06:28<02:00,  1.33s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  78%|███████▊  | 310/400 [06:30<02:02,  1.36s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  78%|███████▊  | 311/400 [06:31<02:00,  1.35s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  78%|███████▊  | 312/400 [06:32<01:55,  1.32s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  78%|███████▊  | 313/400 [06:33<01:52,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  78%|███████▊  | 314/400 [06:35<01:49,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  79%|███████▉  | 315/400 [06:36<01:46,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  79%|███████▉  | 316/400 [06:37<01:44,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  79%|███████▉  | 317/400 [06:38<01:43,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  80%|███████▉  | 318/400 [06:39<01:40,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [182, 396, 379, 43, 255, 348, 363, 34, 377, 375, 248]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [182. 396. 379.  43. 255. 348. 363.  34. 377. 375. 248.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       272.72726\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               128.11932\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             6.69 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            6.15 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009991678\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -70.38353\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 371.5\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 398.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 371.5\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 348.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 8\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04133322293922646\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.24729873534573668\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09060242920524929\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  80%|███████▉  | 319/400 [06:41<01:39,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  80%|████████  | 320/400 [06:42<01:37,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  80%|████████  | 321/400 [06:43<01:37,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  80%|████████  | 322/400 [06:45<01:38,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  81%|████████  | 323/400 [06:46<01:38,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  81%|████████  | 324/400 [06:47<01:35,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  81%|████████▏ | 325/400 [06:48<01:33,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  82%|████████▏ | 326/400 [06:50<01:37,  1.32s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  82%|████████▏ | 327/400 [06:51<01:34,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  82%|████████▏ | 328/400 [06:52<01:31,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  82%|████████▏ | 329/400 [06:53<01:29,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  82%|████████▎ | 330/400 [06:55<01:27,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  83%|████████▎ | 331/400 [06:56<01:25,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  83%|████████▎ | 332/400 [06:57<01:24,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  83%|████████▎ | 333/400 [06:58<01:22,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  84%|████████▎ | 334/400 [07:00<01:21,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  84%|████████▍ | 335/400 [07:01<01:19,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  84%|████████▍ | 336/400 [07:02<01:18,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  84%|████████▍ | 337/400 [07:03<01:17,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  84%|████████▍ | 338/400 [07:04<01:15,  1.22s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  85%|████████▍ | 339/400 [07:06<01:14,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  85%|████████▌ | 340/400 [07:07<01:13,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  85%|████████▌ | 341/400 [07:08<01:12,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  86%|████████▌ | 342/400 [07:09<01:11,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  86%|████████▌ | 343/400 [07:11<01:10,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  86%|████████▌ | 344/400 [07:12<01:08,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  86%|████████▋ | 345/400 [07:13<01:07,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  86%|████████▋ | 346/400 [07:14<01:06,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  87%|████████▋ | 347/400 [07:15<01:04,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  87%|████████▋ | 348/400 [07:17<01:07,  1.31s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  87%|████████▋ | 349/400 [07:18<01:05,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  88%|████████▊ | 350/400 [07:19<01:03,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  88%|████████▊ | 351/400 [07:21<01:02,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  88%|████████▊ | 352/400 [07:22<01:00,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  88%|████████▊ | 353/400 [07:23<00:58,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  88%|████████▊ | 354/400 [07:24<00:57,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  89%|████████▉ | 355/400 [07:26<00:55,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  89%|████████▉ | 356/400 [07:27<00:54,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  89%|████████▉ | 357/400 [07:28<00:53,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  90%|████████▉ | 358/400 [07:29<00:51,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [19, 310, 296, 300, 75, 159, 295, 277, 269, 62, 315, 293, 297, 33]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [ 19. 310. 296. 300.  75. 159. 295. 277. 269.  62. 315. 293. 297.  33.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       214.28572\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               112.267815\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             7.52 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            6.92 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.0099933725\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -64.61014\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 293.1818181818182\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 315.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 293.1818181818182\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 277.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 11\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04125238290371725\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.24704360032536998\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09037903419301727\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  90%|████████▉ | 359/400 [07:31<00:50,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  90%|█████████ | 360/400 [07:32<00:49,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  90%|█████████ | 361/400 [07:33<00:49,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  90%|█████████ | 362/400 [07:34<00:48,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  91%|█████████ | 363/400 [07:36<00:46,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  91%|█████████ | 364/400 [07:37<00:47,  1.31s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  91%|█████████▏| 365/400 [07:39<00:46,  1.34s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  92%|█████████▏| 366/400 [07:40<00:45,  1.33s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  92%|█████████▏| 367/400 [07:41<00:42,  1.30s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  92%|█████████▏| 368/400 [07:42<00:41,  1.31s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  92%|█████████▏| 369/400 [07:44<00:40,  1.30s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  92%|█████████▎| 370/400 [07:45<00:38,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  93%|█████████▎| 371/400 [07:46<00:36,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  93%|█████████▎| 372/400 [07:47<00:35,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  93%|█████████▎| 373/400 [07:49<00:33,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  94%|█████████▎| 374/400 [07:50<00:32,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  94%|█████████▍| 375/400 [07:51<00:31,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  94%|█████████▍| 376/400 [07:52<00:29,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  94%|█████████▍| 377/400 [07:54<00:28,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  94%|█████████▍| 378/400 [07:55<00:27,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  95%|█████████▍| 379/400 [07:56<00:25,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  95%|█████████▌| 380/400 [07:57<00:24,  1.23s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  95%|█████████▌| 381/400 [07:59<00:24,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  96%|█████████▌| 382/400 [08:00<00:23,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  96%|█████████▌| 383/400 [08:01<00:21,  1.27s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  96%|█████████▌| 384/400 [08:02<00:20,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  96%|█████████▋| 385/400 [08:04<00:19,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  96%|█████████▋| 386/400 [08:05<00:17,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  97%|█████████▋| 387/400 [08:06<00:16,  1.29s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  97%|█████████▋| 388/400 [08:08<00:15,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  97%|█████████▋| 389/400 [08:09<00:14,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  98%|█████████▊| 390/400 [08:10<00:12,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  98%|█████████▊| 391/400 [08:11<00:11,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  98%|█████████▊| 392/400 [08:13<00:10,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  98%|█████████▊| 393/400 [08:14<00:08,  1.26s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  98%|█████████▊| 394/400 [08:15<00:07,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  99%|█████████▉| 395/400 [08:16<00:06,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  99%|█████████▉| 396/400 [08:18<00:04,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m  99%|█████████▉| 397/400 [08:19<00:03,  1.24s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m 100%|█████████▉| 398/400 [08:20<00:02,  1.25s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \u001b[2K\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampled steps:                            3000\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Episode Lengths:                          [75, 290, 303, 299, 33, 108, 309, 293, 290, 274, 298, 311, 117]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sum of rewards for episodes:              [ 75. 290. 303. 299.  33. 108. 309. 293. 290. 274. 298. 311. 117.]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Average sum of rewards per episode:       230.76923\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Std of rewards per episode:               100.4223\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Time elapsed:                             8.36 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Sampling time:                            7.70 mins\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m KL between old and new distribution:      0.009980309\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m Surrogate loss:                           -64.91514\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_len_mean: 299.6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_max: 314.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_mean: 299.6\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episode_reward_min: 286.0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m episodes_this_iter: 10\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m num_metric_batches_dropped: 0\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m sampler_perf:\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_env_wait_ms: 0.04140863252439414\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_inference_ms: 0.24789859808651635\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   mean_processing_ms: 0.09076412059172409\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m 100%|█████████▉| 399/400 [08:21<00:01,  1.24s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-22 18:53:37,065\tINFO ray_trial_executor.py:180 -- Destroying actor for trial training_workflow_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for training_workflow_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-22_18-53-37\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: d8cb662e2719425786c9d5269d8d97df\n",
      "  hostname: MD-NOTE\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.100\n",
      "  num_metric_batches_dropped: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 12882\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 503.24697828292847\n",
      "  time_this_iter_s: 503.24697828292847\n",
      "  time_total_s: 503.24697828292847\n",
      "  timestamp: 1558540417\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.8/8.2 GB\n",
      "Result logdir: /home/denklewer/ray_results/training_workflow\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - training_workflow_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=12882], 503 s, 1 iter, nan rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m 100%|██████████| 400/400 [08:23<00:00,  1.28s/it]\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m /home/denklewer/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m /home/denklewer/.local/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=12882)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.8/8.2 GB\n",
      "Result logdir: /home/denklewer/ray_results/training_workflow\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - training_workflow_0:\tTERMINATED, [4 CPUs, 0 GPUs], [pid=12882], 503 s, 1 iter, nan rew\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[training_workflow_0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tune.run(\n",
    "        training_workflow,\n",
    "        resources_per_trial={\n",
    "            \"gpu\": 0,\n",
    "            \"cpu\": 3,\n",
    "            \"extra_cpu\": 1,\n",
    "        },\n",
    "        config={\n",
    "            \"num_workers\": 3,\n",
    "            \"num_iters\": 400,\n",
    "\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "colab_type": "code",
    "id": "-k7DmyvCI_WJ",
    "outputId": "4dc10bdf-700c-43ec-d0a8-2d66852e49ec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc98c174e80>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open ('/home/denklewer/Documents/master_test/lunar_rew_hist_1_2000_200.txt', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)\n",
    "    fig,((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=[8,8])\n",
    "    ax1.plot(itemlist ,label='rewards')\n",
    "    ax1.set_title(\"Session rewards\")\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights = []\n",
    "with open ('/home/denklewer/Documents/master_test/LunarLander-v2_weights_2_500_200.txt', 'rb') as fp:\n",
    "    saved_weights = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4mVio2bEROV"
   },
   "outputs": [],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "from collections import OrderedDict\n",
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "eval_steps_cnt = 300\n",
    "\n",
    "def eval_workflow(config, reporter):\n",
    "    env_name = \"LunarLander-v2\"\n",
    "    # Setup policy and policy evaluation actors\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    policy = CustomPolicy(env.observation_space, env.action_space, {})\n",
    "    workers = [\n",
    "        PolicyEvaluator.as_remote().remote(\n",
    "            lambda c: gym.wrappers.Monitor(gym.make(env_name), \"/home/denklewer/Documents/master_test/\", force=True),\n",
    "            CustomPolicy,\n",
    "            batch_steps=eval_steps_cnt,\n",
    "            model_config= MODEL_DEFAULTS,\n",
    "        )\n",
    "        for _ in range(config[\"num_workers\"])\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    reward_history = []\n",
    "    std_history = []\n",
    "    sampling_time = 0\n",
    "    stats_step = config[\"num_iters\"] / 10\n",
    "    for it in trange(config[\"num_iters\"]):\n",
    "        # Broadcast weights to the policy evaluation workers\n",
    "        saved_weights = config[\"saved_weights\"]\n",
    "        print(type(saved_weights))\n",
    "        weights = ray.put({\"default_policy\": config[\"saved_weights\"]})\n",
    "        for w in workers:\n",
    "            w.set_weights.remote(weights)\n",
    "        start_sampling = time.time()\n",
    "\n",
    "        # Gather a batch of samples\n",
    "        samples = SampleBatch.concat_samples(ray.get([w.sample.remote() for w in workers]))\n",
    "        \n",
    "        sampling_time += (time.time() - start_sampling)\n",
    "        loss_stats =   policy.learn_on_batch(samples)\n",
    "        paths = samples.split_by_episode()\n",
    "        episode_lengths = [len(path[SampleBatch.ACTIONS]) for path in paths]\n",
    "        episode_sum_rewards = np.array([path[SampleBatch.REWARDS].sum() for path in paths])\n",
    "        \n",
    "        reward_history.append(episode_sum_rewards.mean())\n",
    "        std_history.append(episode_sum_rewards.std())\n",
    "        result =collect_metrics(remote_evaluators=workers)\n",
    "        if (it+2) % stats_step == 0:\n",
    "            stats = OrderedDict()\n",
    "            stats[\"Sampled steps\"] = len(samples[SampleBatch.ACTIONS])\n",
    "            stats[\"Episode Lengths\"] = episode_lengths\n",
    "            stats[\"Sum of rewards for episodes\"] = episode_sum_rewards\n",
    "            stats[\"Average sum of rewards per episode\"] = episode_sum_rewards.mean()\n",
    "            stats[\"Std of rewards per episode\"] = episode_sum_rewards.std()\n",
    "            stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
    "            stats[\"Sampling time\"] = \"%.2f mins\" % ((sampling_time)/60.)\n",
    "            stats[\"KL between old and new distribution\"] = loss_stats['kl'].data.numpy()\n",
    "            stats[\"Surrogate loss\"] = loss_stats['loss'].data.numpy()\n",
    "            clear_output(True)\n",
    "            for k, v in stats.items():\n",
    "                print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n",
    "            print(pretty_print(result))\n",
    "            saved_weights = policy.get_weights()\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "#         prev_t = 0\n",
    "#         paths = []\n",
    "#         counter = 0\n",
    "#         obervations, actions, rewards, action_probs, cum_returns = [], [], [], [], []\n",
    "#         for t in samples['t']:\n",
    "#           if t == 0 and prev_t != 0:\n",
    "            \n",
    "#             path = {\"observations\": np.array(obervations),\n",
    "#                     \"policy\": np.array(action_probs),\n",
    "#                     \"actions\": np.array(actions),\n",
    "#                     \"rewards\": np.array(rewards),\n",
    "#                     \"cumulative_returns\": np.array(cum_returns),\n",
    "#                     }\n",
    "#             obervations, actions, rewards, action_probs = [], [], [], []\n",
    "#             paths.append(path)\n",
    "#           else:  \n",
    "#             obervations.append(samples['obs'][counter])\n",
    "#             actions.append(samples['actions'][counter])\n",
    "#             action_probs.append(samples['action_probs'][counter])\n",
    "#             rewards.append(samples['rewards'][counter])   \n",
    "#             cum_returns.append(samples['cummulative_returns'][counter])\n",
    "#             prev_t = t\n",
    "#           counter+=1\n",
    "\n",
    "        # Improve the policy using the  batch\n",
    "\n",
    " \n",
    "    reporter(**collect_metrics(remote_evaluators=workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tune.run(\n",
    "        eval_workflow,\n",
    "        resources_per_trial={\n",
    "            \"gpu\": 0,\n",
    "            \"cpu\": 1,\n",
    "            \"extra_cpu\": 1,\n",
    "        },\n",
    "        config={\n",
    "            \"num_workers\": 1,\n",
    "            \"num_iters\": 2,\n",
    "            \"monitor\": True,\n",
    "            \"saved_weights\": saved_weights\n",
    "\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "trpo-ray.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
